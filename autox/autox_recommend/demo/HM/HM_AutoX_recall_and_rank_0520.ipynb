{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T23:25:14.116304Z",
     "start_time": "2022-05-20T23:25:14.112213Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../AutoX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T23:25:20.082810Z",
     "start_time": "2022-05-20T23:25:14.832375Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "from autox.autox_recommend.recall_and_rank.feature_engineer import feature_engineer\n",
    "from autox.autox_recommend.recall_and_rank.ranker import ranker, ranker_test, inference\n",
    "from autox.autox_recommend.recall_and_rank.recalls import binary_recall\n",
    "from autox.autox_recommend.recall_and_rank.recalls import history_recall\n",
    "from autox.autox_recommend.recall_and_rank.recalls import itemcf_recall\n",
    "from autox.autox_recommend.recall_and_rank.recalls import popular_recall\n",
    "from autox.autox_recommend.recall_and_rank.recalls import w2v_concent_recall\n",
    "from autox.autox_recommend.metrics import mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T23:25:20.215431Z",
     "start_time": "2022-05-20T23:25:20.213216Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from autox import AutoXRecommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T23:25:20.322847Z",
     "start_time": "2022-05-20T23:25:20.320117Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T23:25:20.441754Z",
     "start_time": "2022-05-20T23:25:20.439379Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '/home/caihengxing/kaggle/h_and_m/input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T23:26:11.101798Z",
     "start_time": "2022-05-20T23:25:21.205741Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inter_df = pd.read_csv(path + 'transactions_train.csv', dtype={'article_id': str})\n",
    "user_df = pd.read_csv(path + 'customers.csv')\n",
    "item_df = pd.read_csv(path + 'articles.csv', dtype={'article_id': str})\n",
    "sample_submission = pd.read_csv(path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T23:26:12.914176Z",
     "start_time": "2022-05-20T23:26:12.900851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0663713001</td>\n",
       "      <td>0.050831</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0541518023</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>0505221004</td>\n",
       "      <td>0.015237</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>0685687003</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>0685687004</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        t_dat                                        customer_id  article_id  \\\n",
       "0  2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0663713001   \n",
       "1  2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0541518023   \n",
       "2  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0505221004   \n",
       "3  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0685687003   \n",
       "4  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0685687004   \n",
       "\n",
       "      price  sales_channel_id  \n",
       "0  0.050831                 2  \n",
       "1  0.030492                 2  \n",
       "2  0.015237                 2  \n",
       "3  0.016932                 2  \n",
       "4  0.016932                 2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T23:26:14.725549Z",
     "start_time": "2022-05-20T23:26:14.637547Z"
    }
   },
   "outputs": [],
   "source": [
    "class AutoXRecommend():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, inter_df, user_df, item_df,\n",
    "                  uid, iid, time_col,\n",
    "                  recall_num,\n",
    "                  time_decay=0.8,\n",
    "                  debug=False, debug_save_path=None):\n",
    "\n",
    "        self.inter_df = inter_df\n",
    "        self.user_df = user_df\n",
    "        self.item_df = item_df\n",
    "\n",
    "        self.uid = uid\n",
    "        self.iid = iid\n",
    "        self.time_col = time_col\n",
    "        self.recall_num = recall_num\n",
    "        self.time_decay = time_decay\n",
    "        self.debug = debug\n",
    "\n",
    "        if debug:\n",
    "            assert debug_save_path is not None\n",
    "            path_output = debug_save_path\n",
    "            self.path_output = path_output\n",
    "            os.makedirs(path_output, exist_ok=True)\n",
    "\n",
    "        temp_date = datetime.datetime.strptime(str(inter_df[time_col].max()), '%Y-%m-%d %H:%M:%S') + \\\n",
    "                    datetime.timedelta(days=1)\n",
    "        valid_date = str(datetime.datetime(temp_date.year, temp_date.month, temp_date.day))\n",
    "        self.valid_date = valid_date\n",
    "\n",
    "        train_date = datetime.datetime.strptime(valid_date, '%Y-%m-%d %H:%M:%S') - datetime.timedelta(days=7)\n",
    "        train_date = str(train_date)\n",
    "\n",
    "\n",
    "        print('\\npopular_recall')\n",
    "        print('train')\n",
    "        popular_recall_train = popular_recall(None, inter_df, date=train_date,\n",
    "                                              uid=uid, iid=iid, time_col=time_col,\n",
    "                                              last_days=7, recall_num=recall_num, dtype='train')\n",
    "        print('valid')\n",
    "        popular_recall_valid = popular_recall(None, inter_df, date=valid_date,\n",
    "                                              uid=uid, iid=iid, time_col=time_col,\n",
    "                                              last_days=7, recall_num=recall_num, dtype='train')\n",
    "\n",
    "        print('\\nhistory_recall')\n",
    "        print('train')\n",
    "        history_recall_train = history_recall(None, inter_df, date=train_date,\n",
    "                                              uid=uid, iid=iid, time_col=time_col,\n",
    "                                              last_days=7, recall_num=recall_num, dtype='train')\n",
    "        print('valid')\n",
    "        history_recall_valid = history_recall(None, inter_df, date=valid_date,\n",
    "                                              uid=uid, iid=iid, time_col=time_col,\n",
    "                                              last_days=7, recall_num=recall_num, dtype='train')\n",
    "\n",
    "        print('\\nitemcf_recall')\n",
    "        print('train')\n",
    "        if os.path.exists(f'{path_output}/itemcf_recall_train.hdf'):\n",
    "            itemcf_recall_train = pd.read_hdf(f'{path_output}/itemcf_recall_train.hdf')\n",
    "        else:\n",
    "            itemcf_recall_train = itemcf_recall(None, inter_df, date=train_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, recall_num=recall_num, dtype='train',\n",
    "                                                topk=1000, use_iif=False, sim_last_days=14,\n",
    "                                                time_decay=time_decay)\n",
    "        print('valid')\n",
    "        if os.path.exists(f'{path_output}/itemcf_recall_valid.hdf'):\n",
    "            itemcf_recall_valid = pd.read_hdf(f'{path_output}/itemcf_recall_valid.hdf')\n",
    "        else:\n",
    "            itemcf_recall_valid = itemcf_recall(None, inter_df, date=valid_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, recall_num=recall_num, dtype='train',\n",
    "                                                topk=1000, use_iif=False, sim_last_days=14,\n",
    "                                                time_decay=time_decay)\n",
    "        if debug:\n",
    "            itemcf_recall_train.to_hdf(f'{path_output}/itemcf_recall_train.hdf', 'w', complib='blosc', complevel=5)\n",
    "            itemcf_recall_valid.to_hdf(f'{path_output}/itemcf_recall_valid.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        print('\\nbinary_recall')\n",
    "        print('train')\n",
    "        if os.path.exists(f'{path_output}/binary_recall_train.hdf'):\n",
    "            binary_recall_train = pd.read_hdf(f'{path_output}/binary_recall_train.hdf')\n",
    "        else:\n",
    "            binary_recall_train = binary_recall(None, inter_df, date=train_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, recall_num=recall_num, dtype='train', topk=1000)\n",
    "\n",
    "        print('valid')\n",
    "        if os.path.exists(f'{path_output}/binary_recall_valid.hdf'):\n",
    "            binary_recall_valid = pd.read_hdf(f'{path_output}/binary_recall_valid.hdf')\n",
    "        else:\n",
    "            binary_recall_valid = binary_recall(None, inter_df, date=valid_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, recall_num=recall_num, dtype='train', topk=1000)\n",
    "        if debug:\n",
    "            binary_recall_train.to_hdf(f'{path_output}/binary_recall_train.hdf', 'w', complib='blosc', complevel=5)\n",
    "            binary_recall_valid.to_hdf(f'{path_output}/binary_recall_valid.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        print('\\nw2v_content_recall')\n",
    "        print('train')\n",
    "        if os.path.exists(f'{path_output}/w2v_content_recall_train.hdf'):\n",
    "            w2v_content_recall_train = pd.read_hdf(f'{path_output}/w2v_content_recall_train.hdf')\n",
    "        else:\n",
    "            w2v_content_recall_train = w2v_concent_recall(None, inter_df, date=train_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, dtype='train',\n",
    "                                                topn = 20, topk = 20, prefix = 'w2v')\n",
    "\n",
    "        print('valid')\n",
    "        if os.path.exists(f'{path_output}/w2v_content_recall_valid.hdf'):\n",
    "            w2v_content_recall_valid = pd.read_hdf(f'{path_output}/w2v_content_recall_valid.hdf')\n",
    "        else:\n",
    "            w2v_content_recall_valid = w2v_concent_recall(None, inter_df, date=valid_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, dtype='train',\n",
    "                                                topn=20, topk=20, prefix='w2v')\n",
    "        if debug:\n",
    "            w2v_content_recall_train.to_hdf(f'{path_output}/w2v_content_recall_train.hdf', 'w', complib='blosc', complevel=5)\n",
    "            w2v_content_recall_valid.to_hdf(f'{path_output}/w2v_content_recall_valid.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        # 合并召回数据\n",
    "        print('\\nmerge recalls')\n",
    "        print('train')\n",
    "        history_recall_train.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        itemcf_recall_train.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        binary_recall_train.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        train = popular_recall_train.append(history_recall_train)\n",
    "        train.drop_duplicates(subset=[uid, iid], keep='first', inplace=True)\n",
    "        train = train.merge(itemcf_recall_train, on=[uid, iid, 'label'], how='outer')\n",
    "        train = train.merge(binary_recall_train, on=[uid, iid, 'label'], how='outer')\n",
    "        train = train.merge(w2v_content_recall_train, on=[uid, iid, 'label'], how='outer')\n",
    "\n",
    "        print('valid')\n",
    "        history_recall_valid.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        itemcf_recall_valid.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        binary_recall_valid.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        valid = popular_recall_valid.append(history_recall_valid)\n",
    "        valid.drop_duplicates(subset=[uid, iid], keep='first', inplace=True)\n",
    "        valid = valid.merge(itemcf_recall_valid, on=[uid, iid, 'label'], how='outer')\n",
    "        valid = valid.merge(binary_recall_valid, on=[uid, iid, 'label'], how='outer')\n",
    "        valid = valid.merge(w2v_content_recall_valid, on=[uid, iid, 'label'], how='outer')\n",
    "\n",
    "        # 特征工程\n",
    "        print('\\nfeature engineer')\n",
    "        print('train')\n",
    "        if os.path.exists(f'{path_output}/train_fe.hdf'):\n",
    "            train_fe = pd.read_hdf(f'{path_output}/train_fe.hdf')\n",
    "        else:\n",
    "            train_fe = feature_engineer(train, inter_df,\n",
    "                                        date=train_date,\n",
    "                                        user_df=user_df, item_df=item_df,\n",
    "                                        uid=uid, iid=iid, time_col=time_col,\n",
    "                                        last_days=7, dtype='train')\n",
    "        print('valid')\n",
    "        if os.path.exists(f'{path_output}/valid_fe.hdf'):\n",
    "            valid_fe = pd.read_hdf(f'{path_output}/valid_fe.hdf')\n",
    "        else:\n",
    "            valid_fe = feature_engineer(valid, inter_df,\n",
    "                                        date=valid_date,\n",
    "                                        user_df=user_df, item_df=item_df,\n",
    "                                        uid=uid, iid=iid, time_col=time_col,\n",
    "                                        last_days=7, dtype='train')\n",
    "\n",
    "        if debug:\n",
    "            train_fe.to_hdf(f'{path_output}/train_fe.hdf', 'w', complib='blosc', complevel=5)\n",
    "            valid_fe.to_hdf(f'{path_output}/valid_fe.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "\n",
    "        iid2idx = {}\n",
    "        idx2iid = {}\n",
    "        for idx, cur_iid in enumerate(train_fe[iid].unique()):\n",
    "            iid2idx[cur_iid] = idx\n",
    "            idx2iid[idx] = cur_iid\n",
    "        self.iid2idx = iid2idx\n",
    "        train_fe[iid + '_idx'] = train_fe[iid].map(iid2idx)\n",
    "        valid_fe[iid + '_idx'] = valid_fe[iid].map(iid2idx)\n",
    "\n",
    "        print(f\"train_fe shape: {train_fe.shape}\")\n",
    "        print(f\"valid_fe shape: {valid_fe.shape}\")\n",
    "\n",
    "        print('\\nranker')\n",
    "        # todo: 检查train_fe中是否有冗余特征, 方差为0的特征\n",
    "\n",
    "        lgb_ranker, valid_pred = ranker(train_fe, valid_fe,\n",
    "                                        uid=uid, iid=iid, time_col=time_col)\n",
    "\n",
    "        print('\\nlocal result calculation')\n",
    "        # 离线结果打印\n",
    "        valid_pred = valid_pred.sort_values('prob', ascending=False)\n",
    "        valid_pred = valid_pred.groupby(uid).head(12).groupby(uid)[iid].agg(list).reset_index()\n",
    "\n",
    "        begin_date = datetime.datetime.strptime(valid_date, '%Y-%m-%d %H:%M:%S') - datetime.timedelta(days=7)\n",
    "        begin_date = str(begin_date)\n",
    "\n",
    "        valid_true = inter_df.loc[inter_df[uid].isin(valid_pred[uid])]\n",
    "        valid_true = valid_true[(valid_true[time_col] <= valid_date) & (valid_true[time_col] > begin_date)]\n",
    "\n",
    "        print(valid_true[time_col].min(), valid_true[time_col].max())\n",
    "        valid_true = valid_true.groupby(uid)[iid].agg(list).reset_index()\n",
    "\n",
    "        print(\"mAP Score on Validation set:\", mapk(valid_true[iid], valid_pred[iid]))\n",
    "\n",
    "        self.best_iteration_ = lgb_ranker.best_iteration_\n",
    "\n",
    "\n",
    "        print(\"#\" * 30)\n",
    "        print('retrain')\n",
    "        # 重新训练\n",
    "        train_date = valid_date\n",
    "        # train_date = '2022-04-07 00:00:00'\n",
    "\n",
    "        print('\\npopular_recall')\n",
    "        popular_recall_train = popular_recall(None, inter_df, date=train_date,\n",
    "                                              uid=uid, iid=iid, time_col=time_col,\n",
    "                                              last_days=7, recall_num=recall_num, dtype='train')\n",
    "        print('\\nhistory_recall')\n",
    "        history_recall_train = history_recall(None, inter_df, date=train_date,\n",
    "                                              uid=uid, iid=iid, time_col=time_col,\n",
    "                                              last_days=7, recall_num=recall_num, dtype='train')\n",
    "        print('\\nitemcf_recall')\n",
    "        if os.path.exists(f'{path_output}/itemcf_recall_train_all.hdf'):\n",
    "            itemcf_recall_train = pd.read_hdf(f'{path_output}/itemcf_recall_train_all.hdf')\n",
    "        else:\n",
    "            itemcf_recall_train = itemcf_recall(None, inter_df, date=train_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, recall_num=recall_num, dtype='train',\n",
    "                                                topk=1000, use_iif=False, sim_last_days=14,\n",
    "                                                time_decay=time_decay)\n",
    "\n",
    "        if debug:\n",
    "            itemcf_recall_train.to_hdf(f'{path_output}/itemcf_recall_train_all.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        print('\\nbinary_recall')\n",
    "        if os.path.exists(f'{path_output}/binary_recall_train_all.hdf'):\n",
    "            binary_recall_train = pd.read_hdf(f'{path_output}/binary_recall_train_all.hdf')\n",
    "        else:\n",
    "            binary_recall_train = binary_recall(None, inter_df, date=train_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, recall_num=recall_num, dtype='train', topk=1000)\n",
    "        if debug:\n",
    "            binary_recall_train.to_hdf(f'{path_output}/binary_recall_train_all.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        print('\\nw2v_content_recall')\n",
    "        if os.path.exists(f'{path_output}/w2v_content_recall_train_all.hdf'):\n",
    "            w2v_content_recall_train = pd.read_hdf(f'{path_output}/w2v_content_recall_train_all.hdf')\n",
    "        else:\n",
    "            w2v_content_recall_train = w2v_concent_recall(None, inter_df, date=train_date,\n",
    "                                                          uid=uid, iid=iid, time_col=time_col,\n",
    "                                                          last_days=7, dtype='train',\n",
    "                                                          topn=20, topk=20, prefix='w2v')\n",
    "        if debug:\n",
    "            w2v_content_recall_train.to_hdf(f'{path_output}/w2v_content_recall_train_all.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "\n",
    "        # 合并召回数据\n",
    "        print('\\nmerge recalls')\n",
    "        history_recall_train.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        itemcf_recall_train.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        binary_recall_train.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        train = popular_recall_train.append(history_recall_train)\n",
    "        train.drop_duplicates(subset=[uid, iid], keep='first', inplace=True)\n",
    "        train = train.merge(itemcf_recall_train, on=[uid, iid, 'label'], how='outer')\n",
    "        train = train.merge(binary_recall_train, on=[uid, iid, 'label'], how='outer')\n",
    "        train = train.merge(w2v_content_recall_train, on=[uid, iid, 'label'], how='outer')\n",
    "\n",
    "        # 特征工程\n",
    "        print('\\nfeature engineer')\n",
    "        if os.path.exists(f'{path_output}/train_fe_all.hdf'):\n",
    "            train_fe = pd.read_hdf(f'{path_output}/train_fe_all.hdf')\n",
    "        else:\n",
    "            train_fe = feature_engineer(train, inter_df,\n",
    "                                        date=train_date,\n",
    "                                        user_df=user_df, item_df=item_df,\n",
    "                                        uid=uid, iid=iid, time_col=time_col,\n",
    "                                        last_days=7, dtype='train')\n",
    "        if debug:\n",
    "            train_fe.to_hdf(f'{path_output}/train_fe_all.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "\n",
    "        train_fe[iid + '_idx'] = train_fe[iid].map(iid2idx)\n",
    "        print(f\"train_fe shape: {train_fe.shape}\")\n",
    "\n",
    "        print('\\nranker')\n",
    "        self.model, self.feats = ranker_test(train_fe, self.best_iteration_,\n",
    "                                   uid=uid, iid=iid, time_col=time_col)\n",
    "\n",
    "\n",
    "    def transform(self, uids):\n",
    "\n",
    "        test_date = self.valid_date\n",
    "        # test_date = '2022-04-07 00:00:00'\n",
    "\n",
    "        print('\\npopular recall, test')\n",
    "        popular_recall_test = popular_recall(uids, self.inter_df, date=test_date,\n",
    "                                             uid=self.uid, iid=self.iid, time_col=self.time_col,\n",
    "                                             last_days=7, recall_num=self.recall_num, dtype='test')\n",
    "        print('\\nhistory recall, test')\n",
    "        history_recall_test = history_recall(uids, self.inter_df, date=test_date,\n",
    "                                             uid=self.uid, iid=self.iid, time_col=self.time_col,\n",
    "                                             last_days=7, recall_num=self.recall_num, dtype='test')\n",
    "        print('\\nitemcf recall, test')\n",
    "        if os.path.exists(f'{self.path_output}/itemcf_recall_test.hdf'):\n",
    "            itemcf_recall_test = pd.read_hdf(f'{self.path_output}/itemcf_recall_test.hdf')\n",
    "        else:\n",
    "            itemcf_recall_test = itemcf_recall(uids, self.inter_df, date=test_date,\n",
    "                                               uid=self.uid, iid=self.iid, time_col=self.time_col,\n",
    "                                               last_days=7, recall_num=self.recall_num, dtype='test',\n",
    "                                               topk=1000, use_iif=False, sim_last_days=14,\n",
    "                                               time_decay=self.time_decay)\n",
    "        if self.debug:\n",
    "            itemcf_recall_test.to_hdf(f'{self.path_output}/itemcf_recall_test.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        print('\\nbinary recall, test')\n",
    "        if os.path.exists(f'{self.path_output}/binary_recall_test.hdf'):\n",
    "            binary_recall_test = pd.read_hdf(f'{self.path_output}/binary_recall_test.hdf')\n",
    "        else:\n",
    "            binary_recall_test = binary_recall(uids, self.inter_df, date=test_date,\n",
    "                                               uid=self.uid, iid=self.iid, time_col=self.time_col,\n",
    "                                               last_days=7, recall_num=self.recall_num, dtype='test', topk=1000)\n",
    "        if self.debug:\n",
    "            binary_recall_test.to_hdf(f'{self.path_output}/binary_recall_test.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        print('\\nw2v_content_recall, test')\n",
    "        if os.path.exists(f'{self.path_output}/w2v_content_recall_test.hdf'):\n",
    "            w2v_content_recall_test = pd.read_hdf(f'{self.path_output}/w2v_content_recall_test.hdf')\n",
    "        else:\n",
    "            w2v_content_recall_test = w2v_concent_recall(uids, self.inter_df, date=test_date,\n",
    "                                                         uid=self.uid, iid=self.iid, time_col=self.time_col,\n",
    "                                                         last_days=7, dtype='test',\n",
    "                                                         topn=20, topk=20, prefix='w2v')\n",
    "        if self.debug:\n",
    "            w2v_content_recall_test.to_hdf(f'{self.path_output}/w2v_content_recall_test.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        print('\\nmerge recalls')\n",
    "        history_recall_test.drop_duplicates(subset=[self.uid, self.iid], keep='first', inplace=True)\n",
    "        itemcf_recall_test.drop_duplicates(subset=[self.uid, self.iid], keep='first', inplace=True)\n",
    "        binary_recall_test.drop_duplicates(subset=[self.uid, self.iid], keep='first', inplace=True)\n",
    "        test = popular_recall_test.append(history_recall_test)\n",
    "        test.drop_duplicates(subset=[self.uid, self.iid], keep='first', inplace=True)\n",
    "        test = test.merge(itemcf_recall_test, on=[self.uid, self.iid], how='outer')\n",
    "        test = test.merge(binary_recall_test, on=[self.uid, self.iid], how='outer')\n",
    "        test = test.merge(w2v_content_recall_test, on=[self.uid, self.iid], how='outer')\n",
    "\n",
    "        print('\\nfeature engineer')\n",
    "        if os.path.exists(f'{self.path_output}/test_fe.hdf'):\n",
    "            test_fe = pd.read_hdf(f'{self.path_output}/test_fe.hdf')\n",
    "        else:\n",
    "            test_fe = feature_engineer(test, self.inter_df,\n",
    "                                       date=test_date,\n",
    "                                       user_df=self.user_df, item_df=self.item_df,\n",
    "                                       uid=self.uid, iid=self.iid, time_col=self.time_col,\n",
    "                                       last_days=7, dtype='test')\n",
    "        # if self.debug:\n",
    "        #     test_fe.to_hdf(f'{self.path_output}/test_fe.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        test_fe[self.iid + '_idx'] = test_fe[self.iid].map(self.iid2idx)\n",
    "        print(f\"test_fe shape: {test_fe.shape}\")\n",
    "\n",
    "        print('\\ninference')\n",
    "        bs = 60000\n",
    "        recs = inference(self.model, self.feats, test_fe, uids,\n",
    "                         uid=self.uid, iid=self.iid, time_col=self.time_col,\n",
    "                         batch_size=bs)\n",
    "\n",
    "        return recs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T23:26:16.460710Z",
     "start_time": "2022-05-20T23:26:16.457467Z"
    }
   },
   "outputs": [],
   "source": [
    "uid = 'customer_id'\n",
    "iid = 'article_id'\n",
    "time_col = 't_dat'\n",
    "recall_num = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意时间列需要转成时间格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T23:26:24.001738Z",
     "start_time": "2022-05-20T23:26:18.209431Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inter_df[time_col] = pd.to_datetime(inter_df[time_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T23:26:25.995194Z",
     "start_time": "2022-05-20T23:26:25.763836Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2018-09-20 00:00:00'), Timestamp('2020-09-22 00:00:00'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_df[time_col].min(), inter_df[time_col].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T23:26:27.655178Z",
     "start_time": "2022-05-20T23:26:27.651705Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31788324, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T23:26:30.567228Z",
     "start_time": "2022-05-20T23:26:29.426880Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "data_used_time = datetime.datetime.strptime('2020-06-22', '%Y-%m-%d')\n",
    "inter_df = inter_df.loc[inter_df[time_col] > data_used_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T23:26:32.246616Z",
     "start_time": "2022-05-20T23:26:32.211901Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2020-06-23 00:00:00'), Timestamp('2020-09-22 00:00:00'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_df[time_col].min(), inter_df[time_col].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T23:26:34.074684Z",
     "start_time": "2022-05-20T23:26:34.071047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3981458, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行AutoXRecommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T00:04:53.764383Z",
     "start_time": "2022-05-20T23:26:35.860206Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "popular_recall\n",
      "train\n",
      "2020-09-10 00:00:00 2020-09-16 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68613/68613 [00:03<00:00, 18469.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIT:  0.054450377315542246\n",
      "valid\n",
      "2020-09-17 00:00:00 2020-09-22 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61950/61950 [00:03<00:00, 17919.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIT:  0.06762614152025388\n",
      "\n",
      "history_recall\n",
      "train\n",
      "2020-09-10 00:00:00 2020-09-16 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48796/48796 [00:01<00:00, 47428.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid\n",
      "2020-09-17 00:00:00 2020-09-22 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44687/44687 [00:00<00:00, 47306.88it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "itemcf_recall\n",
      "train\n",
      "valid\n",
      "\n",
      "binary_recall\n",
      "train\n",
      "valid\n",
      "\n",
      "w2v_content_recall\n",
      "train\n",
      "valid\n",
      "\n",
      "merge recalls\n",
      "train\n",
      "valid\n",
      "\n",
      "feature engineer\n",
      "train\n",
      "valid\n",
      "train_fe shape: (8704358, 62)\n",
      "valid_fe shape: (7928507, 62)\n",
      "\n",
      "ranker\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's map@12: 0.752447\n",
      "[200]\tvalid_0's map@12: 0.753363\n",
      "[300]\tvalid_0's map@12: 0.754015\n",
      "[400]\tvalid_0's map@12: 0.754256\n",
      "[500]\tvalid_0's map@12: 0.754468\n",
      "[600]\tvalid_0's map@12: 0.754554\n",
      "[700]\tvalid_0's map@12: 0.75459\n",
      "[800]\tvalid_0's map@12: 0.754558\n",
      "[900]\tvalid_0's map@12: 0.754766\n",
      "[1000]\tvalid_0's map@12: 0.754916\n",
      "[1100]\tvalid_0's map@12: 0.755056\n",
      "[1200]\tvalid_0's map@12: 0.755124\n",
      "[1300]\tvalid_0's map@12: 0.755181\n",
      "[1400]\tvalid_0's map@12: 0.755171\n",
      "[1500]\tvalid_0's map@12: 0.755278\n",
      "[1600]\tvalid_0's map@12: 0.755244\n",
      "Early stopping, best iteration is:\n",
      "[1500]\tvalid_0's map@12: 0.755278\n",
      "defaultdict(<class 'dict'>, {'valid_0': {'map@12': 0.7552783552988642}})\n",
      "                        feature  importance\n",
      "41               article_id_idx        4166\n",
      "22                 product_code        3620\n",
      "1                  binary_score        3250\n",
      "32  purchase_corr_item_max_time        2920\n",
      "40     latest_purchase_time_sub        2489\n",
      "2         w2v_content_sim_score        2361\n",
      "28                department_no        2280\n",
      "23              product_type_no        2086\n",
      "21                          age        1974\n",
      "31             garment_group_no        1790\n",
      "0                  itemcf_score        1450\n",
      "30                   section_no        1391\n",
      "25            colour_group_code        1209\n",
      "24      graphical_appearance_no        1202\n",
      "10                    price_sum        1167\n",
      "8                     price_std        1056\n",
      "4            n_purchase_nunique         972\n",
      "7                    price_mean         896\n",
      "27   perceived_colour_master_id         819\n",
      "14        sales_channel_id_mean         747\n",
      "\n",
      "local result calculation\n",
      "2020-09-17 00:00:00 2020-09-22 00:00:00\n",
      "mAP Score on Validation set: 0.027831199942766272\n",
      "##############################\n",
      "retrain\n",
      "\n",
      "popular_recall\n",
      "2020-09-17 00:00:00 2020-09-22 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61950/61950 [00:03<00:00, 17087.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIT:  0.06762614152025388\n",
      "\n",
      "history_recall\n",
      "2020-09-17 00:00:00 2020-09-22 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44687/44687 [00:00<00:00, 65766.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "itemcf_recall\n",
      "\n",
      "binary_recall\n",
      "\n",
      "w2v_content_recall\n",
      "\n",
      "merge recalls\n",
      "\n",
      "feature engineer\n",
      "train_fe shape: (7927874, 62)\n",
      "\n",
      "ranker\n",
      "[100]\tvalid_0's map@12: 0.758154\n",
      "[200]\tvalid_0's map@12: 0.758967\n",
      "[300]\tvalid_0's map@12: 0.760189\n",
      "[400]\tvalid_0's map@12: 0.760904\n",
      "[500]\tvalid_0's map@12: 0.761908\n",
      "[600]\tvalid_0's map@12: 0.762709\n",
      "[700]\tvalid_0's map@12: 0.763572\n",
      "[800]\tvalid_0's map@12: 0.764405\n",
      "[900]\tvalid_0's map@12: 0.765207\n",
      "[1000]\tvalid_0's map@12: 0.765899\n",
      "[1100]\tvalid_0's map@12: 0.766596\n",
      "[1200]\tvalid_0's map@12: 0.767253\n",
      "[1300]\tvalid_0's map@12: 0.76785\n",
      "[1400]\tvalid_0's map@12: 0.768497\n",
      "[1500]\tvalid_0's map@12: 0.769044\n",
      "defaultdict(<class 'dict'>, {'valid_0': {'map@12': 0.769044230098917}})\n",
      "                        feature  importance\n",
      "41               article_id_idx        4356\n",
      "22                 product_code        3555\n",
      "1                  binary_score        3419\n",
      "32  purchase_corr_item_max_time        2801\n",
      "40     latest_purchase_time_sub        2418\n",
      "21                          age        2278\n",
      "2         w2v_content_sim_score        2165\n",
      "28                department_no        2063\n",
      "23              product_type_no        1954\n",
      "0                  itemcf_score        1768\n",
      "31             garment_group_no        1448\n",
      "30                   section_no        1345\n",
      "10                    price_sum        1340\n",
      "25            colour_group_code        1229\n",
      "24      graphical_appearance_no        1201\n",
      "8                     price_std        1150\n",
      "7                    price_mean        1078\n",
      "27   perceived_colour_master_id         909\n",
      "14        sales_channel_id_mean         825\n",
      "9                  price_median         813\n"
     ]
    }
   ],
   "source": [
    "autoXRecommend = AutoXRecommend()\n",
    "\n",
    "autoXRecommend.fit(inter_df = inter_df, user_df = user_df, item_df = item_df,\n",
    "                  uid = uid, iid = iid, time_col = time_col,\n",
    "                  recall_num = recall_num,\n",
    "                  debug = True, debug_save_path = './temp_h_and_m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T00:04:55.487543Z",
     "start_time": "2022-05-21T00:04:55.485205Z"
    }
   },
   "outputs": [],
   "source": [
    "# mAP Score on Validation set: 0.02752116511050258\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T06:42:36.407500Z",
     "start_time": "2022-05-21T00:04:57.241937Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "popular recall, test\n",
      "2020-09-16 00:00:00 2020-09-22 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1371980/1371980 [00:42<00:00, 32011.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "history recall, test\n",
      "\n",
      "itemcf recall, test\n",
      "\n",
      "binary recall, test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42478/42478 [01:26<00:00, 488.52it/s] \n",
      "100%|██████████| 1371980/1371980 [4:50:16<00:00, 78.77it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "w2v_content_recall, test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   INFO -> collecting all words and their counts\n",
      "   INFO -> PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "   INFO -> PROGRESS: at sentence #10000, processed 76194 words, keeping 16026 word types\n",
      "   INFO -> PROGRESS: at sentence #20000, processed 152361 words, keeping 20412 word types\n",
      "   INFO -> PROGRESS: at sentence #30000, processed 228029 words, keeping 22832 word types\n",
      "   INFO -> PROGRESS: at sentence #40000, processed 303985 words, keeping 24701 word types\n",
      "   INFO -> PROGRESS: at sentence #50000, processed 379495 words, keeping 26260 word types\n",
      "   INFO -> PROGRESS: at sentence #60000, processed 455116 words, keeping 27466 word types\n",
      "   INFO -> PROGRESS: at sentence #70000, processed 532013 words, keeping 28621 word types\n",
      "   INFO -> PROGRESS: at sentence #80000, processed 606658 words, keeping 29470 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training w2v model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   INFO -> PROGRESS: at sentence #90000, processed 680899 words, keeping 30224 word types\n",
      "   INFO -> PROGRESS: at sentence #100000, processed 756450 words, keeping 30904 word types\n",
      "   INFO -> PROGRESS: at sentence #110000, processed 831710 words, keeping 31607 word types\n",
      "   INFO -> PROGRESS: at sentence #120000, processed 906131 words, keeping 32219 word types\n",
      "   INFO -> PROGRESS: at sentence #130000, processed 980595 words, keeping 32739 word types\n",
      "   INFO -> PROGRESS: at sentence #140000, processed 1054350 words, keeping 33297 word types\n",
      "   INFO -> PROGRESS: at sentence #150000, processed 1128359 words, keeping 33813 word types\n",
      "   INFO -> PROGRESS: at sentence #160000, processed 1204352 words, keeping 34271 word types\n",
      "   INFO -> PROGRESS: at sentence #170000, processed 1279211 words, keeping 34726 word types\n",
      "   INFO -> PROGRESS: at sentence #180000, processed 1353915 words, keeping 35114 word types\n",
      "   INFO -> PROGRESS: at sentence #190000, processed 1427523 words, keeping 35506 word types\n",
      "   INFO -> PROGRESS: at sentence #200000, processed 1503368 words, keeping 35869 word types\n",
      "   INFO -> PROGRESS: at sentence #210000, processed 1579532 words, keeping 36196 word types\n",
      "   INFO -> PROGRESS: at sentence #220000, processed 1654733 words, keeping 36485 word types\n",
      "   INFO -> PROGRESS: at sentence #230000, processed 1729602 words, keeping 36780 word types\n",
      "   INFO -> PROGRESS: at sentence #240000, processed 1804154 words, keeping 37088 word types\n",
      "   INFO -> PROGRESS: at sentence #250000, processed 1880148 words, keeping 37368 word types\n",
      "   INFO -> PROGRESS: at sentence #260000, processed 1956301 words, keeping 37603 word types\n",
      "   INFO -> PROGRESS: at sentence #270000, processed 2031837 words, keeping 37847 word types\n",
      "   INFO -> PROGRESS: at sentence #280000, processed 2106555 words, keeping 38104 word types\n",
      "   INFO -> PROGRESS: at sentence #290000, processed 2182589 words, keeping 38377 word types\n",
      "   INFO -> PROGRESS: at sentence #300000, processed 2258501 words, keeping 38634 word types\n",
      "   INFO -> PROGRESS: at sentence #310000, processed 2335300 words, keeping 38854 word types\n",
      "   INFO -> PROGRESS: at sentence #320000, processed 2410851 words, keeping 39109 word types\n",
      "   INFO -> PROGRESS: at sentence #330000, processed 2486128 words, keeping 39320 word types\n",
      "   INFO -> PROGRESS: at sentence #340000, processed 2561654 words, keeping 39523 word types\n",
      "   INFO -> PROGRESS: at sentence #350000, processed 2635146 words, keeping 39737 word types\n",
      "   INFO -> PROGRESS: at sentence #360000, processed 2709053 words, keeping 39906 word types\n",
      "   INFO -> PROGRESS: at sentence #370000, processed 2783359 words, keeping 40090 word types\n",
      "   INFO -> PROGRESS: at sentence #380000, processed 2857176 words, keeping 40263 word types\n",
      "   INFO -> PROGRESS: at sentence #390000, processed 2930660 words, keeping 40448 word types\n",
      "   INFO -> PROGRESS: at sentence #400000, processed 3007442 words, keeping 40639 word types\n",
      "   INFO -> PROGRESS: at sentence #410000, processed 3081324 words, keeping 40798 word types\n",
      "   INFO -> PROGRESS: at sentence #420000, processed 3155912 words, keeping 40956 word types\n",
      "   INFO -> PROGRESS: at sentence #430000, processed 3229435 words, keeping 41119 word types\n",
      "   INFO -> PROGRESS: at sentence #440000, processed 3304978 words, keeping 41270 word types\n",
      "   INFO -> PROGRESS: at sentence #450000, processed 3380279 words, keeping 41441 word types\n",
      "   INFO -> PROGRESS: at sentence #460000, processed 3454021 words, keeping 41567 word types\n",
      "   INFO -> PROGRESS: at sentence #470000, processed 3528469 words, keeping 41718 word types\n",
      "   INFO -> PROGRESS: at sentence #480000, processed 3603616 words, keeping 41846 word types\n",
      "   INFO -> PROGRESS: at sentence #490000, processed 3678888 words, keeping 41980 word types\n",
      "   INFO -> PROGRESS: at sentence #500000, processed 3753419 words, keeping 42125 word types\n",
      "   INFO -> PROGRESS: at sentence #510000, processed 3829561 words, keeping 42251 word types\n",
      "   INFO -> PROGRESS: at sentence #520000, processed 3903382 words, keeping 42379 word types\n",
      "   INFO -> PROGRESS: at sentence #530000, processed 3978491 words, keeping 42474 word types\n",
      "   INFO -> collected 42478 word types from a corpus of 3981458 raw words and 530343 sentences\n",
      "   INFO -> Creating a fresh vocabulary\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 42478 unique words (100.00% of original 42478, drops 0)', 'datetime': '2022-05-21T12:58:53.983062', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'prepare_vocab'}\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 3981458 word corpus (100.00% of original 3981458, drops 0)', 'datetime': '2022-05-21T12:58:53.985215', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'prepare_vocab'}\n",
      "   INFO -> deleting the raw counts dictionary of 42478 items\n",
      "   INFO -> sample=0.001 downsamples 0 most-common words\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 3981458 word corpus (100.0%% of prior 3981458)', 'datetime': '2022-05-21T12:58:54.304441', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'prepare_vocab'}\n",
      "   INFO -> estimated required memory for 42478 words and 32 dimensions: 32113368 bytes\n",
      "   INFO -> resetting layer weights\n",
      "   INFO -> Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-05-21T12:58:54.903822', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'build_vocab'}\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'training model with 20 workers on 42478 vocabulary and 32 features, using sg=0 hs=0 sample=0.001 negative=5 window=20 shrink_windows=True', 'datetime': '2022-05-21T12:58:54.905060', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'train'}\n",
      "   INFO -> EPOCH 0 - PROGRESS: at 22.95% examples, 916896 words/s, in_qsize 35, out_qsize 8\n",
      "   INFO -> EPOCH 0 - PROGRESS: at 52.57% examples, 1036449 words/s, in_qsize 40, out_qsize 0\n",
      "   INFO -> EPOCH 0 - PROGRESS: at 78.22% examples, 1016052 words/s, in_qsize 40, out_qsize 0\n",
      "   INFO -> EPOCH 0: training on 3981458 raw words (3981458 effective words) took 3.7s, 1078787 effective words/s\n",
      "   INFO -> EPOCH 1 - PROGRESS: at 22.45% examples, 882743 words/s, in_qsize 40, out_qsize 0\n",
      "   INFO -> EPOCH 1 - PROGRESS: at 49.57% examples, 969840 words/s, in_qsize 39, out_qsize 0\n",
      "   INFO -> EPOCH 1 - PROGRESS: at 76.16% examples, 995580 words/s, in_qsize 40, out_qsize 0\n",
      "   INFO -> EPOCH 1: training on 3981458 raw words (3981458 effective words) took 3.8s, 1052667 effective words/s\n",
      "   INFO -> EPOCH 2 - PROGRESS: at 23.47% examples, 937666 words/s, in_qsize 40, out_qsize 0\n",
      "   INFO -> EPOCH 2 - PROGRESS: at 51.32% examples, 1015494 words/s, in_qsize 40, out_qsize 0\n",
      "   INFO -> EPOCH 2 - PROGRESS: at 79.48% examples, 1039640 words/s, in_qsize 40, out_qsize 0\n",
      "   INFO -> EPOCH 2: training on 3981458 raw words (3981458 effective words) took 3.6s, 1094111 effective words/s\n",
      "   INFO -> EPOCH 3 - PROGRESS: at 22.96% examples, 905376 words/s, in_qsize 34, out_qsize 5\n",
      "   INFO -> EPOCH 3 - PROGRESS: at 49.07% examples, 971028 words/s, in_qsize 38, out_qsize 1\n",
      "   INFO -> EPOCH 3 - PROGRESS: at 76.68% examples, 1011538 words/s, in_qsize 38, out_qsize 1\n",
      "   INFO -> EPOCH 3: training on 3981458 raw words (3981458 effective words) took 3.8s, 1061371 effective words/s\n",
      "   INFO -> EPOCH 4 - PROGRESS: at 22.21% examples, 848426 words/s, in_qsize 38, out_qsize 6\n",
      "   INFO -> EPOCH 4 - PROGRESS: at 48.32% examples, 939553 words/s, in_qsize 40, out_qsize 3\n",
      "   INFO -> EPOCH 4 - PROGRESS: at 74.70% examples, 969191 words/s, in_qsize 38, out_qsize 1\n",
      "   INFO -> EPOCH 4: training on 3981458 raw words (3981458 effective words) took 3.9s, 1028076 effective words/s\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'training on 19907290 raw words (19907290 effective words) took 18.8s, 1058658 effective words/s', 'datetime': '2022-05-21T12:59:13.710358', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'train'}\n",
      "   INFO -> Word2Vec lifecycle event {'params': 'Word2Vec<vocab=42478, vector_size=32, alpha=0.025>', 'datetime': '2022-05-21T12:59:13.711102', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该循环程序运行时间： 21.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.19s/it]\n",
      "100%|██████████| 530343/530343 [02:16<00:00, 3880.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v content recall:  (9142540, 3)\n",
      "\n",
      "merge recalls\n",
      "\n",
      "feature engineer\n",
      "customer feature engineer\n",
      "num_cols: ['price', 'sales_channel_id']\n",
      "cat_cols: []\n",
      "interact feature engineer\n",
      "test_fe shape: (124751319, 61)\n",
      "\n",
      "inference\n",
      "[1/23]\n",
      "(5463386, 61)\n",
      "[2/23]\n",
      "(5449575, 61)\n",
      "[3/23]\n",
      "(5459309, 61)\n",
      "[4/23]\n",
      "(5449241, 61)\n",
      "[5/23]\n",
      "(5467410, 61)\n",
      "[6/23]\n",
      "(5450039, 61)\n",
      "[7/23]\n",
      "(5447103, 61)\n",
      "[8/23]\n",
      "(5462251, 61)\n",
      "[9/23]\n",
      "(5462474, 61)\n",
      "[10/23]\n",
      "(5452483, 61)\n",
      "[11/23]\n",
      "(5465537, 61)\n",
      "[12/23]\n",
      "(5450712, 61)\n",
      "[13/23]\n",
      "(5469452, 61)\n",
      "[14/23]\n",
      "(5449684, 61)\n",
      "[15/23]\n",
      "(5454679, 61)\n",
      "[16/23]\n",
      "(5444054, 61)\n",
      "[17/23]\n",
      "(5471083, 61)\n",
      "[18/23]\n",
      "(5445832, 61)\n",
      "[19/23]\n",
      "(5431956, 61)\n",
      "[20/23]\n",
      "(5454639, 61)\n",
      "[21/23]\n",
      "(5459955, 61)\n",
      "[22/23]\n",
      "(5457461, 61)\n",
      "[23/23]\n",
      "(4733004, 61)\n"
     ]
    }
   ],
   "source": [
    "test_users = list(sample_submission[uid])\n",
    "res = autoXRecommend.transform(test_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T06:42:46.798463Z",
     "start_time": "2022-05-21T06:42:46.787093Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>[0568601043, 0779781015, 0801768001, 079112000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>[0918522001, 0918292001, 0924243001, 075147100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>[0794321007, 0794321011, 0805000007, 092424300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n",
       "      <td>[0924243001, 0918522001, 0751471043, 075147100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n",
       "      <td>[0896152002, 0924243001, 0924243002, 079158701...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   \n",
       "3  00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...   \n",
       "4  00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...   \n",
       "\n",
       "                                          prediction  \n",
       "0  [0568601043, 0779781015, 0801768001, 079112000...  \n",
       "1  [0918522001, 0918292001, 0924243001, 075147100...  \n",
       "2  [0794321007, 0794321011, 0805000007, 092424300...  \n",
       "3  [0924243001, 0918522001, 0751471043, 075147100...  \n",
       "4  [0896152002, 0924243001, 0924243002, 079158701...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T06:43:02.311469Z",
     "start_time": "2022-05-21T06:42:56.283764Z"
    }
   },
   "outputs": [],
   "source": [
    "res['prediction'] = res['prediction'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T06:43:46.117666Z",
     "start_time": "2022-05-21T06:43:13.880709Z"
    }
   },
   "outputs": [],
   "source": [
    "res.to_csv('./AutoX_hm_add_w2v.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T06:45:34.815658Z",
     "start_time": "2022-05-21T06:43:57.506388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: AutoX_hm_add_w2v.csv (deflated 77%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r AutoX_hm_add_w2v.csv.zip AutoX_hm_add_w2v.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T06:45:46.233342Z",
     "start_time": "2022-05-21T06:45:46.229470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1371980, 2), (1371980, 2))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape, sample_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T06:45:57.585956Z",
     "start_time": "2022-05-21T06:45:57.577935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>0568601043 0779781015 0801768001 0791120002 09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>0918522001 0918292001 0924243001 0751471001 09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0794321007 0794321011 0805000007 0924243001 09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n",
       "      <td>0924243001 0918522001 0751471043 0751471001 09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n",
       "      <td>0896152002 0924243001 0924243002 0791587015 07...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   \n",
       "3  00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...   \n",
       "4  00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...   \n",
       "\n",
       "                                          prediction  \n",
       "0  0568601043 0779781015 0801768001 0791120002 09...  \n",
       "1  0918522001 0918292001 0924243001 0751471001 09...  \n",
       "2  0794321007 0794321011 0805000007 0924243001 09...  \n",
       "3  0924243001 0918522001 0751471043 0751471001 09...  \n",
       "4  0896152002 0924243001 0924243002 0791587015 07...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T06:46:08.932436Z",
     "start_time": "2022-05-21T06:46:08.925761Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>0706016001 0706016002 0372860001 0610776002 07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>0706016001 0706016002 0372860001 0610776002 07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0706016001 0706016002 0372860001 0610776002 07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n",
       "      <td>0706016001 0706016002 0372860001 0610776002 07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n",
       "      <td>0706016001 0706016002 0372860001 0610776002 07...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   \n",
       "3  00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...   \n",
       "4  00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...   \n",
       "\n",
       "                                          prediction  \n",
       "0  0706016001 0706016002 0372860001 0610776002 07...  \n",
       "1  0706016001 0706016002 0372860001 0610776002 07...  \n",
       "2  0706016001 0706016002 0372860001 0610776002 07...  \n",
       "3  0706016001 0706016002 0372860001 0610776002 07...  \n",
       "4  0706016001 0706016002 0372860001 0610776002 07...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
