{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T09:58:43.879214Z",
     "start_time": "2022-05-20T09:58:43.876141Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../AutoX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T09:58:49.608415Z",
     "start_time": "2022-05-20T09:58:44.194470Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/home/caihengxing/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "from autox.autox_recommend.recall_and_rank.feature_engineer import feature_engineer\n",
    "from autox.autox_recommend.recall_and_rank.ranker import ranker, ranker_test, inference\n",
    "from autox.autox_recommend.recall_and_rank.recalls import binary_recall\n",
    "from autox.autox_recommend.recall_and_rank.recalls import history_recall\n",
    "from autox.autox_recommend.recall_and_rank.recalls import itemcf_recall\n",
    "from autox.autox_recommend.recall_and_rank.recalls import popular_recall\n",
    "from autox.autox_recommend.recall_and_rank.recalls import w2v_concent_recall\n",
    "from autox.autox_recommend.metrics import mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T09:58:49.613156Z",
     "start_time": "2022-05-20T09:58:49.610828Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from autox import AutoXRecommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T09:58:49.660224Z",
     "start_time": "2022-05-20T09:58:49.615557Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T09:58:49.701686Z",
     "start_time": "2022-05-20T09:58:49.662577Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '~/AutoX/autox/autox_recommend/datasets/MovieLens_AutoX/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T09:59:01.438384Z",
     "start_time": "2022-05-20T09:58:49.703532Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inter_df = pd.read_csv(path + 'inter_df.csv')\n",
    "item_df = pd.read_csv(path + 'item_df.csv')\n",
    "test = pd.read_csv(path + 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T09:59:01.444614Z",
     "start_time": "2022-05-20T09:59:01.441414Z"
    }
   },
   "outputs": [],
   "source": [
    "uid = 'userId'\n",
    "iid = 'movieId'\n",
    "time_col = 'time'\n",
    "recall_num = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备测试集结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T09:59:02.112801Z",
     "start_time": "2022-05-20T09:59:01.446604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1995-01-09 11:46:49', '2019-11-14 23:20:55')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_df[time_col].min(), inter_df[time_col].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T09:59:02.121210Z",
     "start_time": "2022-05-20T09:59:02.115954Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2019-11-15 00:08:42', '2019-11-21 09:06:53')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[time_col].min(), test[time_col].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T09:59:02.434017Z",
     "start_time": "2022-05-20T09:59:02.123573Z"
    }
   },
   "outputs": [],
   "source": [
    "assert(test[time_col].min() > inter_df[time_col].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T09:59:02.455475Z",
     "start_time": "2022-05-20T09:59:02.436069Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "283it [00:00, 175071.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users in testidation: 283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "positive_items_test = test.groupby([uid])[iid].apply(list)\n",
    "test_users = positive_items_test.keys()\n",
    "test_items = []\n",
    "\n",
    "for i, user in tqdm(enumerate(test_users)):\n",
    "    test_items.append(positive_items_test[user])\n",
    "    \n",
    "print(\"Total users in testidation:\", len(test_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时间列转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T09:59:03.404815Z",
     "start_time": "2022-05-20T09:59:02.457412Z"
    }
   },
   "outputs": [],
   "source": [
    "inter_df[time_col] = pd.to_datetime(inter_df[time_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T09:59:03.416693Z",
     "start_time": "2022-05-20T09:59:03.406938Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>2006-05-17 15:34:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>2006-05-17 12:27:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>2006-05-17 15:13:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1237</td>\n",
       "      <td>2006-05-17 12:27:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2632</td>\n",
       "      <td>2006-05-17 15:04:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId                time\n",
       "0       1      296 2006-05-17 15:34:04\n",
       "1       1      307 2006-05-17 12:27:08\n",
       "2       1      665 2006-05-17 15:13:40\n",
       "3       1     1237 2006-05-17 12:27:19\n",
       "4       1     2632 2006-05-17 15:04:08"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行AutoX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T09:59:03.504440Z",
     "start_time": "2022-05-20T09:59:03.418774Z"
    }
   },
   "outputs": [],
   "source": [
    "class AutoXRecommend():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, inter_df, user_df, item_df,\n",
    "                  uid, iid, time_col,\n",
    "                  recall_num,\n",
    "                  time_decay=0.8,\n",
    "                  debug=False, debug_save_path=None):\n",
    "\n",
    "        self.inter_df = inter_df\n",
    "        self.user_df = user_df\n",
    "        self.item_df = item_df\n",
    "\n",
    "        self.uid = uid\n",
    "        self.iid = iid\n",
    "        self.time_col = time_col\n",
    "        self.recall_num = recall_num\n",
    "        self.time_decay = time_decay\n",
    "        self.debug = debug\n",
    "\n",
    "        if debug:\n",
    "            assert debug_save_path is not None\n",
    "            path_output = debug_save_path\n",
    "            self.path_output = path_output\n",
    "            os.makedirs(path_output, exist_ok=True)\n",
    "\n",
    "        temp_date = datetime.datetime.strptime(str(inter_df[time_col].max()), '%Y-%m-%d %H:%M:%S') + \\\n",
    "                    datetime.timedelta(days=1)\n",
    "        valid_date = str(datetime.datetime(temp_date.year, temp_date.month, temp_date.day))\n",
    "        self.valid_date = valid_date\n",
    "\n",
    "        train_date = datetime.datetime.strptime(valid_date, '%Y-%m-%d %H:%M:%S') - datetime.timedelta(days=7)\n",
    "        train_date = str(train_date)\n",
    "\n",
    "\n",
    "        print('\\npopular_recall')\n",
    "        print('train')\n",
    "        popular_recall_train = popular_recall(None, inter_df, date=train_date,\n",
    "                                              uid=uid, iid=iid, time_col=time_col,\n",
    "                                              last_days=7, recall_num=recall_num, dtype='train')\n",
    "        print('valid')\n",
    "        popular_recall_valid = popular_recall(None, inter_df, date=valid_date,\n",
    "                                              uid=uid, iid=iid, time_col=time_col,\n",
    "                                              last_days=7, recall_num=recall_num, dtype='train')\n",
    "\n",
    "        print('\\nhistory_recall')\n",
    "        print('train')\n",
    "        history_recall_train = history_recall(None, inter_df, date=train_date,\n",
    "                                              uid=uid, iid=iid, time_col=time_col,\n",
    "                                              last_days=7, recall_num=recall_num, dtype='train')\n",
    "        print('valid')\n",
    "        history_recall_valid = history_recall(None, inter_df, date=valid_date,\n",
    "                                              uid=uid, iid=iid, time_col=time_col,\n",
    "                                              last_days=7, recall_num=recall_num, dtype='train')\n",
    "\n",
    "        print('\\nitemcf_recall')\n",
    "        print('train')\n",
    "        if os.path.exists(f'{path_output}/itemcf_recall_train.hdf'):\n",
    "            itemcf_recall_train = pd.read_hdf(f'{path_output}/itemcf_recall_train.hdf')\n",
    "        else:\n",
    "            itemcf_recall_train = itemcf_recall(None, inter_df, date=train_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, recall_num=recall_num, dtype='train',\n",
    "                                                topk=1000, use_iif=False, sim_last_days=14,\n",
    "                                                time_decay=time_decay)\n",
    "        print('valid')\n",
    "        if os.path.exists(f'{path_output}/itemcf_recall_valid.hdf'):\n",
    "            itemcf_recall_valid = pd.read_hdf(f'{path_output}/itemcf_recall_valid.hdf')\n",
    "        else:\n",
    "            itemcf_recall_valid = itemcf_recall(None, inter_df, date=valid_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, recall_num=recall_num, dtype='train',\n",
    "                                                topk=1000, use_iif=False, sim_last_days=14,\n",
    "                                                time_decay=time_decay)\n",
    "        if debug:\n",
    "            itemcf_recall_train.to_hdf(f'{path_output}/itemcf_recall_train.hdf', 'w', complib='blosc', complevel=5)\n",
    "            itemcf_recall_valid.to_hdf(f'{path_output}/itemcf_recall_valid.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        print('\\nbinary_recall')\n",
    "        print('train')\n",
    "        if os.path.exists(f'{path_output}/binary_recall_train.hdf'):\n",
    "            binary_recall_train = pd.read_hdf(f'{path_output}/binary_recall_train.hdf')\n",
    "        else:\n",
    "            binary_recall_train = binary_recall(None, inter_df, date=train_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, recall_num=recall_num, dtype='train', topk=1000)\n",
    "\n",
    "        print('valid')\n",
    "        if os.path.exists(f'{path_output}/binary_recall_valid.hdf'):\n",
    "            binary_recall_valid = pd.read_hdf(f'{path_output}/binary_recall_valid.hdf')\n",
    "        else:\n",
    "            binary_recall_valid = binary_recall(None, inter_df, date=valid_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, recall_num=recall_num, dtype='train', topk=1000)\n",
    "        if debug:\n",
    "            binary_recall_train.to_hdf(f'{path_output}/binary_recall_train.hdf', 'w', complib='blosc', complevel=5)\n",
    "            binary_recall_valid.to_hdf(f'{path_output}/binary_recall_valid.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        print('\\nw2v_content_recall')\n",
    "        print('train')\n",
    "        if os.path.exists(f'{path_output}/w2v_content_recall_train.hdf'):\n",
    "            w2v_content_recall_train = pd.read_hdf(f'{path_output}/w2v_content_recall_train.hdf')\n",
    "        else:\n",
    "            w2v_content_recall_train = w2v_concent_recall(None, inter_df, date=train_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, dtype='train',\n",
    "                                                topn = 20, topk = 20, prefix = 'w2v')\n",
    "\n",
    "        print('valid')\n",
    "        if os.path.exists(f'{path_output}/w2v_content_recall_valid.hdf'):\n",
    "            w2v_content_recall_valid = pd.read_hdf(f'{path_output}/w2v_content_recall_valid.hdf')\n",
    "        else:\n",
    "            w2v_content_recall_valid = w2v_concent_recall(None, inter_df, date=valid_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, dtype='train',\n",
    "                                                topn=20, topk=20, prefix='w2v')\n",
    "        if debug:\n",
    "            w2v_content_recall_train.to_hdf(f'{path_output}/w2v_content_recall_train.hdf', 'w', complib='blosc', complevel=5)\n",
    "            w2v_content_recall_valid.to_hdf(f'{path_output}/w2v_content_recall_valid.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        # 合并召回数据\n",
    "        print('\\nmerge recalls')\n",
    "        print('train')\n",
    "        history_recall_train.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        itemcf_recall_train.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        binary_recall_train.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        train = popular_recall_train.append(history_recall_train)\n",
    "        train.drop_duplicates(subset=[uid, iid], keep='first', inplace=True)\n",
    "        train = train.merge(itemcf_recall_train, on=[uid, iid, 'label'], how='outer')\n",
    "        train = train.merge(binary_recall_train, on=[uid, iid, 'label'], how='outer')\n",
    "        train = train.merge(w2v_content_recall_train, on=[uid, iid, 'label'], how='outer')\n",
    "\n",
    "        print('valid')\n",
    "        history_recall_valid.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        itemcf_recall_valid.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        binary_recall_valid.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        valid = popular_recall_valid.append(history_recall_valid)\n",
    "        valid.drop_duplicates(subset=[uid, iid], keep='first', inplace=True)\n",
    "        valid = valid.merge(itemcf_recall_valid, on=[uid, iid, 'label'], how='outer')\n",
    "        valid = valid.merge(binary_recall_valid, on=[uid, iid, 'label'], how='outer')\n",
    "        valid = valid.merge(w2v_content_recall_valid, on=[uid, iid, 'label'], how='outer')\n",
    "\n",
    "        # 特征工程\n",
    "        print('\\nfeature engineer')\n",
    "        print('train')\n",
    "        if os.path.exists(f'{path_output}/train_fe.hdf'):\n",
    "            train_fe = pd.read_hdf(f'{path_output}/train_fe.hdf')\n",
    "        else:\n",
    "            train_fe = feature_engineer(train, inter_df,\n",
    "                                        date=train_date,\n",
    "                                        user_df=user_df, item_df=item_df,\n",
    "                                        uid=uid, iid=iid, time_col=time_col,\n",
    "                                        last_days=7, dtype='train')\n",
    "        print('valid')\n",
    "        if os.path.exists(f'{path_output}/valid_fe.hdf'):\n",
    "            valid_fe = pd.read_hdf(f'{path_output}/valid_fe.hdf')\n",
    "        else:\n",
    "            valid_fe = feature_engineer(valid, inter_df,\n",
    "                                        date=valid_date,\n",
    "                                        user_df=user_df, item_df=item_df,\n",
    "                                        uid=uid, iid=iid, time_col=time_col,\n",
    "                                        last_days=7, dtype='train')\n",
    "\n",
    "        if debug:\n",
    "            train_fe.to_hdf(f'{path_output}/train_fe.hdf', 'w', complib='blosc', complevel=5)\n",
    "            valid_fe.to_hdf(f'{path_output}/valid_fe.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "\n",
    "        iid2idx = {}\n",
    "        idx2iid = {}\n",
    "        for idx, cur_iid in enumerate(train_fe[iid].unique()):\n",
    "            iid2idx[cur_iid] = idx\n",
    "            idx2iid[idx] = cur_iid\n",
    "        self.iid2idx = iid2idx\n",
    "        train_fe[iid + '_idx'] = train_fe[iid].map(iid2idx)\n",
    "        valid_fe[iid + '_idx'] = valid_fe[iid].map(iid2idx)\n",
    "\n",
    "        print(f\"train_fe shape: {train_fe.shape}\")\n",
    "        print(f\"valid_fe shape: {valid_fe.shape}\")\n",
    "\n",
    "        print('\\nranker')\n",
    "        # todo: 检查train_fe中是否有冗余特征, 方差为0的特征\n",
    "\n",
    "        lgb_ranker, valid_pred = ranker(train_fe, valid_fe,\n",
    "                                        uid=uid, iid=iid, time_col=time_col)\n",
    "\n",
    "        print('\\nlocal result calculation')\n",
    "        # 离线结果打印\n",
    "        valid_pred = valid_pred.sort_values('prob', ascending=False)\n",
    "        valid_pred = valid_pred.groupby(uid).head(12).groupby(uid)[iid].agg(list).reset_index()\n",
    "\n",
    "        begin_date = datetime.datetime.strptime(valid_date, '%Y-%m-%d %H:%M:%S') - datetime.timedelta(days=7)\n",
    "        begin_date = str(begin_date)\n",
    "\n",
    "        valid_true = inter_df.loc[inter_df[uid].isin(valid_pred[uid])]\n",
    "        valid_true = valid_true[(valid_true[time_col] <= valid_date) & (valid_true[time_col] > begin_date)]\n",
    "\n",
    "        print(valid_true[time_col].min(), valid_true[time_col].max())\n",
    "        valid_true = valid_true.groupby(uid)[iid].agg(list).reset_index()\n",
    "\n",
    "        print(\"mAP Score on Validation set:\", mapk(valid_true[iid], valid_pred[iid]))\n",
    "\n",
    "        self.best_iteration_ = lgb_ranker.best_iteration_\n",
    "\n",
    "\n",
    "        print(\"#\" * 30)\n",
    "        print('retrain')\n",
    "        # 重新训练\n",
    "        train_date = valid_date\n",
    "        # train_date = '2022-04-07 00:00:00'\n",
    "\n",
    "        print('\\npopular_recall')\n",
    "        popular_recall_train = popular_recall(None, inter_df, date=train_date,\n",
    "                                              uid=uid, iid=iid, time_col=time_col,\n",
    "                                              last_days=7, recall_num=recall_num, dtype='train')\n",
    "        print('\\nhistory_recall')\n",
    "        history_recall_train = history_recall(None, inter_df, date=train_date,\n",
    "                                              uid=uid, iid=iid, time_col=time_col,\n",
    "                                              last_days=7, recall_num=recall_num, dtype='train')\n",
    "        print('\\nitemcf_recall')\n",
    "        if os.path.exists(f'{path_output}/itemcf_recall_train_all.hdf'):\n",
    "            itemcf_recall_train = pd.read_hdf(f'{path_output}/itemcf_recall_train_all.hdf')\n",
    "        else:\n",
    "            itemcf_recall_train = itemcf_recall(None, inter_df, date=train_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, recall_num=recall_num, dtype='train',\n",
    "                                                topk=1000, use_iif=False, sim_last_days=14,\n",
    "                                                time_decay=time_decay)\n",
    "\n",
    "        if debug:\n",
    "            itemcf_recall_train.to_hdf(f'{path_output}/itemcf_recall_train_all.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        print('\\nbinary_recall')\n",
    "        if os.path.exists(f'{path_output}/binary_recall_train_all.hdf'):\n",
    "            binary_recall_train = pd.read_hdf(f'{path_output}/binary_recall_train_all.hdf')\n",
    "        else:\n",
    "            binary_recall_train = binary_recall(None, inter_df, date=train_date,\n",
    "                                                uid=uid, iid=iid, time_col=time_col,\n",
    "                                                last_days=7, recall_num=recall_num, dtype='train', topk=1000)\n",
    "        if debug:\n",
    "            binary_recall_train.to_hdf(f'{path_output}/binary_recall_train_all.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        print('\\nw2v_content_recall')\n",
    "        if os.path.exists(f'{path_output}/w2v_content_recall_train_all.hdf'):\n",
    "            w2v_content_recall_train = pd.read_hdf(f'{path_output}/w2v_content_recall_train_all.hdf')\n",
    "        else:\n",
    "            w2v_content_recall_train = w2v_concent_recall(None, inter_df, date=train_date,\n",
    "                                                          uid=uid, iid=iid, time_col=time_col,\n",
    "                                                          last_days=7, dtype='train',\n",
    "                                                          topn=20, topk=20, prefix='w2v')\n",
    "        if debug:\n",
    "            w2v_content_recall_train.to_hdf(f'{path_output}/w2v_content_recall_train_all.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "\n",
    "        # 合并召回数据\n",
    "        print('\\nmerge recalls')\n",
    "        history_recall_train.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        itemcf_recall_train.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        binary_recall_train.drop_duplicates(subset=[uid, iid, 'label'], keep='first', inplace=True)\n",
    "        train = popular_recall_train.append(history_recall_train)\n",
    "        train.drop_duplicates(subset=[uid, iid], keep='first', inplace=True)\n",
    "        train = train.merge(itemcf_recall_train, on=[uid, iid, 'label'], how='outer')\n",
    "        train = train.merge(binary_recall_train, on=[uid, iid, 'label'], how='outer')\n",
    "        train = train.merge(w2v_content_recall_train, on=[uid, iid, 'label'], how='outer')\n",
    "\n",
    "        # 特征工程\n",
    "        print('\\nfeature engineer')\n",
    "        if os.path.exists(f'{path_output}/train_fe_all.hdf'):\n",
    "            train_fe = pd.read_hdf(f'{path_output}/train_fe_all.hdf')\n",
    "        else:\n",
    "            train_fe = feature_engineer(train, inter_df,\n",
    "                                        date=train_date,\n",
    "                                        user_df=user_df, item_df=item_df,\n",
    "                                        uid=uid, iid=iid, time_col=time_col,\n",
    "                                        last_days=7, dtype='train')\n",
    "        if debug:\n",
    "            train_fe.to_hdf(f'{path_output}/train_fe_all.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "\n",
    "        train_fe[iid + '_idx'] = train_fe[iid].map(iid2idx)\n",
    "        print(f\"train_fe shape: {train_fe.shape}\")\n",
    "\n",
    "        print('\\nranker')\n",
    "        self.model, self.feats = ranker_test(train_fe, self.best_iteration_,\n",
    "                                   uid=uid, iid=iid, time_col=time_col)\n",
    "\n",
    "\n",
    "    def transform(self, uids):\n",
    "\n",
    "        test_date = self.valid_date\n",
    "        # test_date = '2022-04-07 00:00:00'\n",
    "\n",
    "        print('\\npopular recall, test')\n",
    "        popular_recall_test = popular_recall(uids, self.inter_df, date=test_date,\n",
    "                                             uid=self.uid, iid=self.iid, time_col=self.time_col,\n",
    "                                             last_days=7, recall_num=self.recall_num, dtype='test')\n",
    "        print('\\nhistory recall, test')\n",
    "        history_recall_test = history_recall(uids, self.inter_df, date=test_date,\n",
    "                                             uid=self.uid, iid=self.iid, time_col=self.time_col,\n",
    "                                             last_days=7, recall_num=self.recall_num, dtype='test')\n",
    "        print('\\nitemcf recall, test')\n",
    "        if os.path.exists(f'{self.path_output}/itemcf_recall_test.hdf'):\n",
    "            itemcf_recall_test = pd.read_hdf(f'{self.path_output}/itemcf_recall_test.hdf')\n",
    "        else:\n",
    "            itemcf_recall_test = itemcf_recall(uids, self.inter_df, date=test_date,\n",
    "                                               uid=self.uid, iid=self.iid, time_col=self.time_col,\n",
    "                                               last_days=7, recall_num=self.recall_num, dtype='test',\n",
    "                                               topk=1000, use_iif=False, sim_last_days=14,\n",
    "                                               time_decay=self.time_decay)\n",
    "        if self.debug:\n",
    "            itemcf_recall_test.to_hdf(f'{self.path_output}/itemcf_recall_test.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        print('\\nbinary recall, test')\n",
    "        if os.path.exists(f'{self.path_output}/binary_recall_test.hdf'):\n",
    "            binary_recall_test = pd.read_hdf(f'{self.path_output}/binary_recall_test.hdf')\n",
    "        else:\n",
    "            binary_recall_test = binary_recall(uids, self.inter_df, date=test_date,\n",
    "                                               uid=self.uid, iid=self.iid, time_col=self.time_col,\n",
    "                                               last_days=7, recall_num=self.recall_num, dtype='test', topk=1000)\n",
    "        if self.debug:\n",
    "            binary_recall_test.to_hdf(f'{self.path_output}/binary_recall_test.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        print('\\nw2v_content_recall, test')\n",
    "        if os.path.exists(f'{self.path_output}/w2v_content_recall_test.hdf'):\n",
    "            w2v_content_recall_test = pd.read_hdf(f'{self.path_output}/w2v_content_recall_test.hdf')\n",
    "        else:\n",
    "            w2v_content_recall_test = w2v_concent_recall(uids, self.inter_df, date=test_date,\n",
    "                                                         uid=self.uid, iid=self.iid, time_col=self.time_col,\n",
    "                                                         last_days=7, dtype='test',\n",
    "                                                         topn=20, topk=20, prefix='w2v')\n",
    "        if self.debug:\n",
    "            w2v_content_recall_test.to_hdf(f'{self.path_output}/w2v_content_recall_test.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        print('\\nmerge recalls')\n",
    "        history_recall_test.drop_duplicates(subset=[self.uid, self.iid], keep='first', inplace=True)\n",
    "        itemcf_recall_test.drop_duplicates(subset=[self.uid, self.iid], keep='first', inplace=True)\n",
    "        binary_recall_test.drop_duplicates(subset=[self.uid, self.iid], keep='first', inplace=True)\n",
    "        test = popular_recall_test.append(history_recall_test)\n",
    "        test.drop_duplicates(subset=[self.uid, self.iid], keep='first', inplace=True)\n",
    "        test = test.merge(itemcf_recall_test, on=[self.uid, self.iid], how='outer')\n",
    "        test = test.merge(binary_recall_test, on=[self.uid, self.iid], how='outer')\n",
    "        test = test.merge(w2v_content_recall_test, on=[self.uid, self.iid], how='outer')\n",
    "\n",
    "        print('\\nfeature engineer')\n",
    "        if os.path.exists(f'{self.path_output}/test_fe.hdf'):\n",
    "            test_fe = pd.read_hdf(f'{self.path_output}/test_fe.hdf')\n",
    "        else:\n",
    "            test_fe = feature_engineer(test, self.inter_df,\n",
    "                                       date=test_date,\n",
    "                                       user_df=self.user_df, item_df=self.item_df,\n",
    "                                       uid=self.uid, iid=self.iid, time_col=self.time_col,\n",
    "                                       last_days=7, dtype='test')\n",
    "        # if self.debug:\n",
    "        #     test_fe.to_hdf(f'{self.path_output}/test_fe.hdf', 'w', complib='blosc', complevel=5)\n",
    "\n",
    "        test_fe[self.iid + '_idx'] = test_fe[self.iid].map(self.iid2idx)\n",
    "        print(f\"test_fe shape: {test_fe.shape}\")\n",
    "\n",
    "        print('\\ninference')\n",
    "        bs = 60000\n",
    "        recs = inference(self.model, self.feats, test_fe, uids,\n",
    "                         uid=self.uid, iid=self.iid, time_col=self.time_col,\n",
    "                         batch_size=bs)\n",
    "\n",
    "        return recs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T10:49:03.157325Z",
     "start_time": "2022-05-20T09:59:03.506359Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "popular_recall\n",
      "train\n",
      "2019-11-01 00:21:32 2019-11-07 23:45:48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 331/331 [00:00<00:00, 1085.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIT:  0.27881950171515285\n",
      "valid\n",
      "2019-11-08 00:01:28 2019-11-14 23:20:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:00<00:00, 21320.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIT:  0.27728749446707573\n",
      "\n",
      "history_recall\n",
      "train\n",
      "2019-11-01 00:21:32 2019-11-07 23:45:48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:00<00:00, 58693.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid\n",
      "2019-11-08 00:01:28 2019-11-14 23:20:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 213/213 [00:00<00:00, 50983.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "itemcf_recall\n",
      "train\n",
      "2019-11-01 00:21:32 2019-11-07 23:45:48\n",
      "calculate similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154714/154714 [07:09<00:00, 360.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemCF recommend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [01:50<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22100, 3)\n",
      "ItemCF recall:  (22100, 4)\n",
      "mean: 0.0028054298642533936\n",
      "sum: 62.0\n",
      "valid\n",
      "2019-11-08 00:01:28 2019-11-14 23:20:55\n",
      "calculate similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154824/154824 [07:10<00:00, 359.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemCF recommend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213/213 [01:43<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21300, 3)\n",
      "ItemCF recall:  (21300, 4)\n",
      "mean: 0.002347417840375587\n",
      "sum: 50.0\n",
      "\n",
      "binary_recall\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27005/27005 [05:05<00:00, 88.28it/s] \n",
      "100%|██████████| 331/331 [01:45<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22100, 3)\n",
      "BinaryNet recall:  (22100, 4)\n",
      "0.0032126696832579186\n",
      "valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27045/27045 [05:06<00:00, 88.26it/s] \n",
      "100%|██████████| 310/310 [01:37<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21300, 3)\n",
      "BinaryNet recall:  (21300, 4)\n",
      "0.0032863849765258214\n",
      "\n",
      "w2v_content_recall\n",
      "train\n",
      "2019-11-01 00:21:32 2019-11-07 23:45:48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   INFO -> collecting all words and their counts\n",
      "   INFO -> PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "   INFO -> collected 11786 word types from a corpus of 86213 raw words and 4838 sentences\n",
      "   INFO -> Creating a fresh vocabulary\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 11786 unique words (100.00% of original 11786, drops 0)', 'datetime': '2022-05-20T18:31:05.894252', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'prepare_vocab'}\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 86213 word corpus (100.00% of original 86213, drops 0)', 'datetime': '2022-05-20T18:31:05.896024', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'prepare_vocab'}\n",
      "   INFO -> deleting the raw counts dictionary of 11786 items\n",
      "   INFO -> sample=0.001 downsamples 51 most-common words\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 81040.60190811759 word corpus (94.0%% of prior 86213)', 'datetime': '2022-05-20T18:31:05.985916', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training w2v model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   INFO -> estimated required memory for 11786 words and 32 dimensions: 8910216 bytes\n",
      "   INFO -> resetting layer weights\n",
      "   INFO -> Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-05-20T18:31:06.158625', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'build_vocab'}\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'training model with 20 workers on 11786 vocabulary and 32 features, using sg=0 hs=0 sample=0.001 negative=5 window=20 shrink_windows=True', 'datetime': '2022-05-20T18:31:06.159229', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'train'}\n",
      "   INFO -> EPOCH 0: training on 86213 raw words (80958 effective words) took 0.1s, 1072652 effective words/s\n",
      "   INFO -> EPOCH 1: training on 86213 raw words (81075 effective words) took 0.1s, 1226258 effective words/s\n",
      "   INFO -> EPOCH 2: training on 86213 raw words (81002 effective words) took 0.1s, 1139153 effective words/s\n",
      "   INFO -> EPOCH 3: training on 86213 raw words (80963 effective words) took 0.1s, 1171143 effective words/s\n",
      "   INFO -> EPOCH 4: training on 86213 raw words (81047 effective words) took 0.1s, 1367646 effective words/s\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'training on 431065 raw words (405045 effective words) took 0.4s, 1006518 effective words/s', 'datetime': '2022-05-20T18:31:06.562207', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'train'}\n",
      "   INFO -> Word2Vec lifecycle event {'params': 'Word2Vec<vocab=11786, vector_size=32, alpha=0.025>', 'datetime': '2022-05-20T18:31:06.562929', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该循环程序运行时间： 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n",
      "100%|██████████| 331/331 [00:00<00:00, 857.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v content recall:  (4420, 4) 0.0011312217194570137\n",
      "valid\n",
      "2019-11-08 00:01:28 2019-11-14 23:20:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   INFO -> collecting all words and their counts\n",
      "   INFO -> PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "   INFO -> collected 11901 word types from a corpus of 86378 raw words and 4829 sentences\n",
      "   INFO -> Creating a fresh vocabulary\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 11901 unique words (100.00% of original 11901, drops 0)', 'datetime': '2022-05-20T18:31:07.767693', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'prepare_vocab'}\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 86378 word corpus (100.00% of original 86378, drops 0)', 'datetime': '2022-05-20T18:31:07.768808', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'prepare_vocab'}\n",
      "   INFO -> deleting the raw counts dictionary of 11901 items\n",
      "   INFO -> sample=0.001 downsamples 50 most-common words\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 81340.72075144862 word corpus (94.2%% of prior 86378)', 'datetime': '2022-05-20T18:31:07.855579', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training w2v model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   INFO -> estimated required memory for 11901 words and 32 dimensions: 8997156 bytes\n",
      "   INFO -> resetting layer weights\n",
      "   INFO -> Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-05-20T18:31:08.029398', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'build_vocab'}\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'training model with 20 workers on 11901 vocabulary and 32 features, using sg=0 hs=0 sample=0.001 negative=5 window=20 shrink_windows=True', 'datetime': '2022-05-20T18:31:08.030267', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'train'}\n",
      "   INFO -> EPOCH 0: training on 86378 raw words (81388 effective words) took 0.1s, 1246117 effective words/s\n",
      "   INFO -> EPOCH 1: training on 86378 raw words (81287 effective words) took 0.1s, 1237563 effective words/s\n",
      "   INFO -> EPOCH 2: training on 86378 raw words (81343 effective words) took 0.1s, 1357464 effective words/s\n",
      "   INFO -> EPOCH 3: training on 86378 raw words (81343 effective words) took 0.1s, 1372222 effective words/s\n",
      "   INFO -> EPOCH 4: training on 86378 raw words (81288 effective words) took 0.1s, 1380024 effective words/s\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'training on 431890 raw words (406649 effective words) took 0.4s, 1127714 effective words/s', 'datetime': '2022-05-20T18:31:08.391628', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'train'}\n",
      "   INFO -> Word2Vec lifecycle event {'params': 'Word2Vec<vocab=11901, vector_size=32, alpha=0.025>', 'datetime': '2022-05-20T18:31:08.392262', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该循环程序运行时间： 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      "100%|██████████| 310/310 [00:00<00:00, 893.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v content recall:  (4240, 4) 0.0011792452830188679\n",
      "\n",
      "merge recalls\n",
      "train\n",
      "valid\n",
      "\n",
      "feature engineer\n",
      "train\n",
      "customer feature engineer\n",
      "interact feature engineer\n",
      "valid\n",
      "customer feature engineer\n",
      "interact feature engineer\n",
      "train_fe shape: (77262, 1148)\n",
      "valid_fe shape: (71990, 1148)\n",
      "\n",
      "ranker\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's map@12: 0.489323\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's map@12: 0.490511\n",
      "defaultdict(<class 'dict'>, {'valid_0': {'map@12': 0.4905106166448333}})\n",
      "                          feature  importance\n",
      "3                      n_purchase         260\n",
      "1141     latest_purchase_time_sub         224\n",
      "0                    itemcf_score          80\n",
      "1133  purchase_corr_item_max_time          79\n",
      "1                    binary_score          76\n",
      "4              n_purchase_nunique          58\n",
      "1142                  movieId_idx          46\n",
      "945                       tag_941          20\n",
      "767                       tag_763          19\n",
      "758                       tag_754          17\n",
      "1134       purchase_corr_item_cnt          16\n",
      "385                       tag_381          11\n",
      "996                       tag_992          10\n",
      "812                       tag_808           9\n",
      "64                         tag_60           8\n",
      "44                         tag_40           8\n",
      "756                       tag_752           8\n",
      "278                       tag_274           8\n",
      "209                       tag_205           8\n",
      "699                       tag_695           8\n",
      "\n",
      "local result calculation\n",
      "2019-11-08 00:01:28 2019-11-14 23:20:55\n",
      "mAP Score on Validation set: 0.08013039709105203\n",
      "##############################\n",
      "retrain\n",
      "\n",
      "popular_recall\n",
      "2019-11-08 00:01:28 2019-11-14 23:20:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:00<00:00, 20854.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIT:  0.27728749446707573\n",
      "\n",
      "history_recall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-08 00:01:28 2019-11-14 23:20:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213/213 [00:00<00:00, 48736.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "itemcf_recall\n",
      "2019-11-08 00:01:28 2019-11-14 23:20:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154824/154824 [07:11<00:00, 358.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemCF recommend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213/213 [01:43<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21300, 3)\n",
      "ItemCF recall:  (21300, 4)\n",
      "mean: 0.002347417840375587\n",
      "sum: 50.0\n",
      "\n",
      "binary_recall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27045/27045 [05:06<00:00, 88.20it/s] \n",
      "100%|██████████| 310/310 [01:37<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21300, 3)\n",
      "BinaryNet recall:  (21300, 4)\n",
      "0.0032863849765258214\n",
      "\n",
      "w2v_content_recall\n",
      "2019-11-08 00:01:28 2019-11-14 23:20:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   INFO -> collecting all words and their counts\n",
      "   INFO -> PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "   INFO -> collected 11901 word types from a corpus of 86378 raw words and 4829 sentences\n",
      "   INFO -> Creating a fresh vocabulary\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 11901 unique words (100.00% of original 11901, drops 0)', 'datetime': '2022-05-20T18:48:17.133277', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'prepare_vocab'}\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 86378 word corpus (100.00% of original 86378, drops 0)', 'datetime': '2022-05-20T18:48:17.134167', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'prepare_vocab'}\n",
      "   INFO -> deleting the raw counts dictionary of 11901 items\n",
      "   INFO -> sample=0.001 downsamples 50 most-common words\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 81340.72075144862 word corpus (94.2%% of prior 86378)', 'datetime': '2022-05-20T18:48:17.230056', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training w2v model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   INFO -> estimated required memory for 11901 words and 32 dimensions: 8997156 bytes\n",
      "   INFO -> resetting layer weights\n",
      "   INFO -> Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-05-20T18:48:17.410740', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'build_vocab'}\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'training model with 20 workers on 11901 vocabulary and 32 features, using sg=0 hs=0 sample=0.001 negative=5 window=20 shrink_windows=True', 'datetime': '2022-05-20T18:48:17.411476', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'train'}\n",
      "   INFO -> EPOCH 0: training on 86378 raw words (81324 effective words) took 0.1s, 1144130 effective words/s\n",
      "   INFO -> EPOCH 1: training on 86378 raw words (81284 effective words) took 0.1s, 1148340 effective words/s\n",
      "   INFO -> EPOCH 2: training on 86378 raw words (81330 effective words) took 0.1s, 1267501 effective words/s\n",
      "   INFO -> EPOCH 3: training on 86378 raw words (81357 effective words) took 0.1s, 1235055 effective words/s\n",
      "   INFO -> EPOCH 4: training on 86378 raw words (81294 effective words) took 0.1s, 1155751 effective words/s\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'training on 431890 raw words (406589 effective words) took 0.4s, 1030123 effective words/s', 'datetime': '2022-05-20T18:48:17.807028', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'train'}\n",
      "   INFO -> Word2Vec lifecycle event {'params': 'Word2Vec<vocab=11901, vector_size=32, alpha=0.025>', 'datetime': '2022-05-20T18:48:17.807585', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该循环程序运行时间： 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n",
      "100%|██████████| 310/310 [00:00<00:00, 930.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v content recall:  (4240, 4) 0.0018867924528301887\n",
      "\n",
      "merge recalls\n",
      "\n",
      "feature engineer\n",
      "customer feature engineer\n",
      "interact feature engineer\n",
      "train_fe shape: (71998, 1148)\n",
      "\n",
      "ranker\n",
      "defaultdict(<class 'dict'>, {'valid_0': {'map@12': 0.5444415510235316}})\n",
      "                          feature  importance\n",
      "3                      n_purchase         245\n",
      "1141     latest_purchase_time_sub         216\n",
      "1133  purchase_corr_item_max_time          84\n",
      "4              n_purchase_nunique          73\n",
      "1                    binary_score          69\n",
      "0                    itemcf_score          62\n",
      "1142                  movieId_idx          43\n",
      "1134       purchase_corr_item_cnt          18\n",
      "996                       tag_992          18\n",
      "1095                     tag_1091          12\n",
      "25                         tag_21          12\n",
      "511                       tag_507          10\n",
      "688                       tag_684          10\n",
      "340                       tag_336          10\n",
      "1128                     tag_1124           9\n",
      "921                       tag_917           8\n",
      "58                         tag_54           8\n",
      "772                       tag_768           7\n",
      "625                       tag_621           7\n",
      "615                       tag_611           7\n"
     ]
    }
   ],
   "source": [
    "autoXRecommend = AutoXRecommend()\n",
    "\n",
    "autoXRecommend.fit(inter_df = inter_df, user_df = None, item_df = item_df,\n",
    "                  uid = uid, iid = iid, time_col = time_col,\n",
    "                  recall_num = recall_num,\n",
    "                  time_decay = 0.99,\n",
    "                  debug = True, debug_save_path = './temp_MovieLens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T11:04:54.663956Z",
     "start_time": "2022-05-20T10:49:03.160370Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "popular recall, test\n",
      "2019-11-08 00:01:28 2019-11-14 23:20:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:00<00:00, 44956.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "history recall, test\n",
      "\n",
      "itemcf recall, test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 154921/154921 [07:14<00:00, 356.44it/s]\n",
      "100%|██████████| 202/202 [01:31<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "binary recall, test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27102/27102 [05:06<00:00, 88.44it/s] \n",
      "100%|██████████| 283/283 [01:27<00:00,  3.25it/s]\n",
      "   INFO -> collecting all words and their counts\n",
      "   INFO -> PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "   INFO -> collected 11901 word types from a corpus of 86378 raw words and 4829 sentences\n",
      "   INFO -> Creating a fresh vocabulary\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 11901 unique words (100.00% of original 11901, drops 0)', 'datetime': '2022-05-20T19:04:38.976913', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'prepare_vocab'}\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 86378 word corpus (100.00% of original 86378, drops 0)', 'datetime': '2022-05-20T19:04:38.977892', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "w2v_content_recall, test\n",
      "Begin training w2v model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   INFO -> deleting the raw counts dictionary of 11901 items\n",
      "   INFO -> sample=0.001 downsamples 50 most-common words\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 81340.72075144862 word corpus (94.2%% of prior 86378)', 'datetime': '2022-05-20T19:04:39.074374', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'prepare_vocab'}\n",
      "   INFO -> estimated required memory for 11901 words and 32 dimensions: 8997156 bytes\n",
      "   INFO -> resetting layer weights\n",
      "   INFO -> Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-05-20T19:04:39.263848', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'build_vocab'}\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'training model with 20 workers on 11901 vocabulary and 32 features, using sg=0 hs=0 sample=0.001 negative=5 window=20 shrink_windows=True', 'datetime': '2022-05-20T19:04:39.264608', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'train'}\n",
      "   INFO -> EPOCH 0: training on 86378 raw words (81316 effective words) took 0.1s, 1039346 effective words/s\n",
      "   INFO -> EPOCH 1: training on 86378 raw words (81401 effective words) took 0.1s, 1136974 effective words/s\n",
      "   INFO -> EPOCH 2: training on 86378 raw words (81290 effective words) took 0.1s, 1150100 effective words/s\n",
      "   INFO -> EPOCH 3: training on 86378 raw words (81210 effective words) took 0.1s, 1076913 effective words/s\n",
      "   INFO -> EPOCH 4: training on 86378 raw words (81377 effective words) took 0.1s, 1278865 effective words/s\n",
      "   INFO -> Word2Vec lifecycle event {'msg': 'training on 431890 raw words (406594 effective words) took 0.4s, 973475 effective words/s', 'datetime': '2022-05-20T19:04:39.682972', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'train'}\n",
      "   INFO -> Word2Vec lifecycle event {'params': 'Word2Vec<vocab=11901, vector_size=32, alpha=0.025>', 'datetime': '2022-05-20T19:04:39.683666', 'gensim': '4.2.0', 'python': '3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]', 'platform': 'Linux-3.10.0-1127.8.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该循环程序运行时间： 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      "100%|██████████| 202/202 [00:00<00:00, 581.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v content recall:  (4020, 3)\n",
      "\n",
      "merge recalls\n",
      "\n",
      "feature engineer\n",
      "customer feature engineer\n",
      "interact feature engineer\n",
      "test_fe shape: (67754, 1147)\n",
      "\n",
      "inference\n",
      "[1/1]\n",
      "(67754, 1147)\n"
     ]
    }
   ],
   "source": [
    "res = autoXRecommend.transform(test_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T11:04:54.672401Z",
     "start_time": "2022-05-20T11:04:54.666132Z"
    }
   },
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=12):\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=12):\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T11:04:54.751707Z",
     "start_time": "2022-05-20T11:04:54.674260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP Score on Validation set: 0.07350586162911206\n"
     ]
    }
   ],
   "source": [
    "outputs = res['prediction']\n",
    "print(\"mAP Score on Validation set:\", mapk(test_items, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T11:04:54.798941Z",
     "start_time": "2022-05-20T11:04:54.753701Z"
    }
   },
   "outputs": [],
   "source": [
    "# mAP Score on Validation set: 0.08030425675382308\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
