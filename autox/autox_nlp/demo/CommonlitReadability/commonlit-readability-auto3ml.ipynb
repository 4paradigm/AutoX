{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import KFold\n\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:51:04.140903Z","iopub.execute_input":"2022-05-08T13:51:04.141160Z","iopub.status.idle":"2022-05-08T13:51:04.211817Z","shell.execute_reply.started":"2022-05-08T13:51:04.141131Z","shell.execute_reply":"2022-05-08T13:51:04.211111Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=3, random_state=1001,shuffle=True)\nfor i, (train_index, val_index) in enumerate(kf.split(train)):\n    trn= train.iloc[train_index].reset_index()\n    val= train.iloc[val_index].reset_index()\n    \ntrn = trn.drop(columns=['index'])\nval = val.drop(columns=['index'])\n\nval.to_csv('sub_val.csv',index=False)\ntrn.to_csv('sub_train.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:51:04.213422Z","iopub.execute_input":"2022-05-08T13:51:04.213830Z","iopub.status.idle":"2022-05-08T13:51:04.353711Z","shell.execute_reply.started":"2022-05-08T13:51:04.213794Z","shell.execute_reply":"2022-05-08T13:51:04.352970Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# !rm -r ./autox\n!git clone https://github.com/4paradigm/autox.git\n!pip install ./autox","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-08T13:51:04.355761Z","iopub.execute_input":"2022-05-08T13:51:04.356162Z","iopub.status.idle":"2022-05-08T13:51:42.020237Z","shell.execute_reply.started":"2022-05-08T13:51:04.356126Z","shell.execute_reply":"2022-05-08T13:51:42.019368Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Cloning into 'autox'...\nremote: Enumerating objects: 2699, done.\u001b[K\nremote: Counting objects: 100% (612/612), done.\u001b[K\nremote: Compressing objects: 100% (153/153), done.\u001b[K\nremote: Total 2699 (delta 479), reused 570 (delta 452), pack-reused 2087\u001b[K\nReceiving objects: 100% (2699/2699), 11.87 MiB | 26.03 MiB/s, done.\nResolving deltas: 100% (1723/1723), done.\nProcessing ./autox\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: lightgbm in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (3.3.1)\nRequirement already satisfied: xgboost in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (1.6.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (1.9.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (1.21.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (1.3.5)\nRequirement already satisfied: sklearn in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (4.63.0)\nRequirement already satisfied: optuna in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (2.10.0)\nCollecting img2vec_pytorch\n  Downloading img2vec_pytorch-1.0.1-py3-none-any.whl (6.9 kB)\nCollecting pypinyin\n  Downloading pypinyin-0.46.0-py2.py3-none-any.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: keras in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (2.6.0)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (2.6.3)\nRequirement already satisfied: gensim in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (4.0.1)\nCollecting glove-python-binary\n  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.9/948.9 KB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (4.18.0)\nCollecting datasets\n  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.4/325.4 KB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (4.11.3)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (3.8.1)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (2022.3.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (0.5.1)\nCollecting xxhash\n  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting responses<0.19\n  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (21.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (0.3.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (2.27.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (0.70.12.2)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (7.0.0)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim->automl-x==0.3.0) (5.2.1)\nRequirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim->automl-x==0.3.0) (1.7.3)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from img2vec_pytorch->automl-x==0.3.0) (0.10.1)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from lightgbm->automl-x==0.3.0) (0.37.1)\nRequirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.7/site-packages (from lightgbm->automl-x==0.3.0) (1.0.2)\nRequirement already satisfied: alembic in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (1.7.7)\nRequirement already satisfied: cmaes>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (0.8.2)\nRequirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (1.4.32)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (6.6.0)\nRequirement already satisfied: cliff in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (3.10.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (6.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->automl-x==0.3.0) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->automl-x==0.3.0) (2021.3)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (5.0)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (0.4.0)\nRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (2.6.0)\nRequirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (2.6.0)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (3.3.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (1.1.2)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (1.43.0)\nCollecting wrapt~=1.12.1\n  Downloading wrapt-1.12.1.tar.gz (27 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting typing-extensions<3.11,>=3.7\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (0.2.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (1.6.3)\nCollecting six~=1.15.0\n  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (1.12)\nCollecting absl-py~=0.10\n  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (3.19.4)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (1.1.0)\nRequirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (3.1.0)\nCollecting numpy\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers->automl-x==0.3.0) (2021.11.10)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers->automl-x==0.3.0) (0.0.49)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers->automl-x==0.3.0) (0.12.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers->automl-x==0.3.0) (3.6.0)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow->automl-x==0.3.0) (1.5.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets->automl-x==0.3.0) (3.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->automl-x==0.3.0) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->automl-x==0.3.0) (1.26.8)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->automl-x==0.3.0) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->automl-x==0.3.0) (2.0.12)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm->automl-x==0.3.0) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm->automl-x==0.3.0) (1.0.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna->automl-x==0.3.0) (1.1.2)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (1.8.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (0.6.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (1.35.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (3.3.6)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (2.0.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (59.8.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (0.4.6)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (1.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (1.7.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (21.4.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (4.0.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (0.13.0)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic->optuna->automl-x==0.3.0) (1.2.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from alembic->optuna->automl-x==0.3.0) (5.4.0)\nRequirement already satisfied: cmd2>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna->automl-x==0.3.0) (2.4.1)\nRequirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna->automl-x==0.3.0) (5.8.1)\nRequirement already satisfied: autopage>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna->automl-x==0.3.0) (0.5.0)\nRequirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna->automl-x==0.3.0) (3.2.0)\nRequirement already satisfied: stevedore>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna->automl-x==0.3.0) (3.5.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets->automl-x==0.3.0) (3.7.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers->automl-x==0.3.0) (8.0.4)\nRequirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->img2vec_pytorch->automl-x==0.3.0) (9.0.1)\nRequirement already satisfied: pyperclip>=1.6 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna->automl-x==0.3.0) (1.8.2)\nRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna->automl-x==0.3.0) (0.2.5)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (4.8)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (1.3.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic->optuna->automl-x==0.3.0) (2.0.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (3.2.0)\nBuilding wheels for collected packages: automl-x, wrapt\n  Building wheel for automl-x (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for automl-x: filename=automl_x-0.3.0-py3-none-any.whl size=4809171 sha256=d55ad0c4dd54df062289b14473ec9e354141d9a7a0ec45294202895090afdfe6\n  Stored in directory: /tmp/pip-ephem-wheel-cache-xk89r5k6/wheels/d8/d0/3f/17c06e69ec61f9c250a3274a013cc8a24b47965ec33f40c73d\n  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=77058 sha256=1e42b7136357a84e4eb0c45314a5d37484614048a742c1644f4467b35c04a0ff\n  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\nSuccessfully built automl-x wrapt\nInstalling collected packages: wrapt, typing-extensions, xxhash, six, pypinyin, numpy, responses, absl-py, img2vec_pytorch, glove-python-binary, datasets, automl-x\n  Attempting uninstall: wrapt\n    Found existing installation: wrapt 1.14.0\n    Uninstalling wrapt-1.14.0:\n      Successfully uninstalled wrapt-1.14.0\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.2.0\n    Uninstalling typing_extensions-4.2.0:\n      Successfully uninstalled typing_extensions-4.2.0\n  Attempting uninstall: six\n    Found existing installation: six 1.16.0\n    Uninstalling six-1.16.0:\n      Successfully uninstalled six-1.16.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: absl-py\n    Found existing installation: absl-py 1.0.0\n    Uninstalling absl-py-1.0.0:\n      Successfully uninstalled absl-py-1.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\ntfx-bsl 1.7.0 requires pyarrow<6,>=1, but you have pyarrow 7.0.0 which is incompatible.\ntfx-bsl 1.7.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5, but you have tensorflow 2.6.3 which is incompatible.\ntensorflow-transform 1.7.0 requires pyarrow<6,>=1, but you have pyarrow 7.0.0 which is incompatible.\ntensorflow-transform 1.7.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<2.9,>=1.15.5, but you have tensorflow 2.6.3 which is incompatible.\ntensorflow-serving-api 2.8.0 requires tensorflow<3,>=2.8.0, but you have tensorflow 2.6.3 which is incompatible.\nrich 12.2.0 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytorch-lightning 1.6.1 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npytools 2022.1.5 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.1 which is incompatible.\nimageio 2.16.1 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\ngrpcio-status 1.44.0 requires grpcio>=1.44.0, but you have grpcio 1.43.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-cloud-storage<2.0.0dev,>=1.26.0, but you have google-cloud-storage 2.1.0 which is incompatible.\ngcsfs 2022.2.0 requires fsspec==2022.02.0, but you have fsspec 2022.3.0 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.11.3 which is incompatible.\nfeaturetools 1.8.0 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\napache-beam 2.37.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\napache-beam 2.37.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.4 which is incompatible.\napache-beam 2.37.0 requires pyarrow<7.0.0,>=0.15.1, but you have pyarrow 7.0.0 which is incompatible.\naioitertools 0.10.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.2.0 requires botocore<1.24.22,>=1.24.21, but you have botocore 1.25.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed absl-py-0.15.0 automl-x-0.3.0 datasets-2.1.0 glove-python-binary-0.2.0 img2vec_pytorch-1.0.1 numpy-1.19.5 pypinyin-0.46.0 responses-0.18.0 six-1.15.0 typing-extensions-3.10.0.2 wrapt-1.12.1 xxhash-3.0.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from autox.autox_nlp import NLP_feature\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:51:42.024325Z","iopub.execute_input":"2022-05-08T13:51:42.024549Z","iopub.status.idle":"2022-05-08T13:51:52.692714Z","shell.execute_reply.started":"2022-05-08T13:51:42.024520Z","shell.execute_reply":"2022-05-08T13:51:52.691987Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"df_train = pd.read_csv('sub_train.csv')\ndf_test = pd.read_csv('sub_val.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:51:52.694160Z","iopub.execute_input":"2022-05-08T13:51:52.694421Z","iopub.status.idle":"2022-05-08T13:51:52.742356Z","shell.execute_reply.started":"2022-05-08T13:51:52.694375Z","shell.execute_reply":"2022-05-08T13:51:52.741697Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"use_Toknizer=True\nemb_mode = 'Bert'# TFIDF / Word2Vec / Glove / FastText / Bert\nencode_mode = 'supervise' # unsupervise / supervise\ntext_columns_name = ['excerpt']\ntarget_column = df_train['target']\ncandidate_labels=None","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:51:52.743613Z","iopub.execute_input":"2022-05-08T13:51:52.743900Z","iopub.status.idle":"2022-05-08T13:51:52.751143Z","shell.execute_reply.started":"2022-05-08T13:51:52.743862Z","shell.execute_reply":"2022-05-08T13:51:52.750196Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"nlp = NLP_feature()\nnlp.do_mlm = True\nnlp.mlm_epochs=3\nnlp.model_name = 'microsoft/deberta-v3-base'\nnlp.emb_size=100\nnlp.n_clusters=20\ndf = nlp.fit(df_train,\n             text_columns_name,\n             use_Toknizer,\n             emb_mode,\n             encode_mode,\n             target_column,\n             candidate_labels)","metadata":{"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-05-08T13:51:52.752678Z","iopub.execute_input":"2022-05-08T13:51:52.752958Z","iopub.status.idle":"2022-05-08T13:58:43.117503Z","shell.execute_reply.started":"2022-05-08T13:51:52.752924Z","shell.execute_reply":"2022-05-08T13:58:43.116762Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Fitting column: excerpt tokenizer\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a455011a82604f4fbc59d77e3eafa0fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f999e0ac4fa4ca1928057d6b568604f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46ea86e5684b475f9b4e2b17634c89b5"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Fitting column: excerpt bert embedding\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/354M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1862f2811c5492c846e8eb418d550ac"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n***** Running training *****\n  Num examples = 1890\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 711\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='711' max='711' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [711/711 03:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>5.744500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nSaving model checkpoint to ./excerpt_transformer\nConfiguration saved in ./excerpt_transformer/config.json\nModel weights saved in ./excerpt_transformer/pytorch_model.bin\nloading configuration file ./excerpt_transformer/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"./excerpt_transformer\",\n  \"architectures\": [\n    \"DebertaV2ForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file ./excerpt_transformer/pytorch_model.bin\n","output_type":"stream"},{"name":"stdout","text":"Fitting column: excerpt encoder\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ./excerpt_transformer were not used when initializing DebertaV2Model: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of DebertaV2Model were initialized from the model checkpoint at ./excerpt_transformer.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2Model for predictions without further training.\nloading configuration file ./excerpt_transformer/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"./excerpt_transformer\",\n  \"architectures\": [\n    \"DebertaV2ForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file ./excerpt_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./excerpt_transformer were not used when initializing DebertaV2Model: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of DebertaV2Model were initialized from the model checkpoint at ./excerpt_transformer.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2Model for predictions without further training.\nloading configuration file ./excerpt_transformer/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"./excerpt_transformer\",\n  \"architectures\": [\n    \"DebertaV2ForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file ./excerpt_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./excerpt_transformer were not used when initializing DebertaV2Model: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of DebertaV2Model were initialized from the model checkpoint at ./excerpt_transformer.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2Model for predictions without further training.\nloading configuration file ./excerpt_transformer/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"./excerpt_transformer\",\n  \"architectures\": [\n    \"DebertaV2ForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file ./excerpt_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./excerpt_transformer were not used when initializing DebertaV2Model: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of DebertaV2Model were initialized from the model checkpoint at ./excerpt_transformer.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2Model for predictions without further training.\nloading configuration file ./excerpt_transformer/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"./excerpt_transformer\",\n  \"architectures\": [\n    \"DebertaV2ForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file ./excerpt_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./excerpt_transformer were not used when initializing DebertaV2Model: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of DebertaV2Model were initialized from the model checkpoint at ./excerpt_transformer.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2Model for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"for column in df.columns:\n    df_train[column] = df[column]\ndf_train = df_train.drop(columns=text_columns_name)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:58:43.119093Z","iopub.execute_input":"2022-05-08T13:58:43.119343Z","iopub.status.idle":"2022-05-08T13:58:43.127853Z","shell.execute_reply.started":"2022-05-08T13:58:43.119295Z","shell.execute_reply":"2022-05-08T13:58:43.127030Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"test = nlp.transform(df_test)","metadata":{"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-05-08T13:58:43.129438Z","iopub.execute_input":"2022-05-08T13:58:43.129849Z","iopub.status.idle":"2022-05-08T13:59:02.175558Z","shell.execute_reply.started":"2022-05-08T13:58:43.129802Z","shell.execute_reply":"2022-05-08T13:59:02.174780Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"loading configuration file ./excerpt_transformer/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"./excerpt_transformer\",\n  \"architectures\": [\n    \"DebertaV2ForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file ./excerpt_transformer/pytorch_model.bin\n","output_type":"stream"},{"name":"stdout","text":"Transforming column: excerpt\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ./excerpt_transformer were not used when initializing DebertaV2Model: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of DebertaV2Model were initialized from the model checkpoint at ./excerpt_transformer.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2Model for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"for column in test.columns:\n    df_test[column] = test[column]\ndf_test = df_test.drop(columns=text_columns_name)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:59:02.178428Z","iopub.execute_input":"2022-05-08T13:59:02.178684Z","iopub.status.idle":"2022-05-08T13:59:02.185788Z","shell.execute_reply.started":"2022-05-08T13:59:02.178650Z","shell.execute_reply":"2022-05-08T13:59:02.184996Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_train.to_csv(f'{emb_mode}_{encode_mode}_autox_trn.csv',index=False)\ndf_test.to_csv(f'{emb_mode}_{encode_mode}_autox_val.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:59:02.187277Z","iopub.execute_input":"2022-05-08T13:59:02.187852Z","iopub.status.idle":"2022-05-08T13:59:02.217574Z","shell.execute_reply.started":"2022-05-08T13:59:02.187812Z","shell.execute_reply":"2022-05-08T13:59:02.216783Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df_val=pd.read_csv(f'{emb_mode}_{encode_mode}_autox_val.csv').drop(columns=['target'])\ndf_val.to_csv(f'{emb_mode}_{encode_mode}_autox_tst.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T13:59:02.219129Z","iopub.execute_input":"2022-05-08T13:59:02.219413Z","iopub.status.idle":"2022-05-08T13:59:02.236741Z","shell.execute_reply.started":"2022-05-08T13:59:02.219375Z","shell.execute_reply":"2022-05-08T13:59:02.236059Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from autox import AutoX\n\npath = f'.' \nautox = AutoX(target = 'target', train_name = f'{emb_mode}_{encode_mode}_autox_trn.csv', test_name = f'{emb_mode}_{encode_mode}_autox_tst.csv',  id = [], path = path)\nsub = autox.get_submit()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-08T13:59:02.238160Z","iopub.execute_input":"2022-05-08T13:59:02.238421Z","iopub.status.idle":"2022-05-08T14:00:58.258054Z","shell.execute_reply.started":"2022-05-08T13:59:02.238388Z","shell.execute_reply":"2022-05-08T14:00:58.257452Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"   INFO ->  [+] read Bert_supervise_autox_trn.csv\n   INFO ->  Memory usage of dataframe is 0.09 MB\n   INFO ->  Memory usage after optimization is: 0.12 MB\n   INFO ->  Decreased by -35.3%\n   INFO ->  table = Bert_supervise_autox_trn.csv, shape = (1890, 6)\n   INFO ->  [+] read Bert_supervise_autox_val.csv\n   INFO ->  Memory usage of dataframe is 0.04 MB\n   INFO ->  Memory usage after optimization is: 0.06 MB\n   INFO ->  Decreased by -35.5%\n   INFO ->  table = Bert_supervise_autox_val.csv, shape = (944, 6)\n   INFO ->  [+] read Bert_supervise_autox_tst.csv\n   INFO ->  Memory usage of dataframe is 0.04 MB\n   INFO ->  Memory usage after optimization is: 0.06 MB\n   INFO ->  Decreased by -57.5%\n   INFO ->  table = Bert_supervise_autox_tst.csv, shape = (944, 5)\n   INFO ->  [+] read sub_train.csv\n   INFO ->  Memory usage of dataframe is 0.09 MB\n   INFO ->  Memory usage after optimization is: 0.19 MB\n   INFO ->  Decreased by -124.7%\n   INFO ->  table = sub_train.csv, shape = (1890, 6)\n   INFO ->  [+] read sub_val.csv\n   INFO ->  Memory usage of dataframe is 0.04 MB\n   INFO ->  Memory usage after optimization is: 0.10 MB\n   INFO ->  Decreased by -124.9%\n   INFO ->  table = sub_val.csv, shape = (944, 6)\n   INFO ->  start feature engineer\n   INFO ->  feature engineer: one2M\n   INFO ->  featureOne2M ops: {}\n   INFO ->  ignore featureOne2M\n   INFO ->  feature engineer: time\n   INFO ->  featureTime ops: []\n0it [00:00, ?it/s]\n   INFO ->  feature engineer: Cumsum\n100%|██████████| 1/1 [00:00<00:00, 167.13it/s]\n   INFO ->  this cols with inf data, del them: ['license__standard_error__cumsum', 'license__excerpt_meta_feature__cumsum']\n   INFO ->  featureCumsum ops: {'license': ['standard_error', 'excerpt_meta_feature']}\n   INFO ->  feature engineer: Shift\n100%|██████████| 1/1 [00:00<00:00, 39.39it/s]\n   INFO ->  featureShift ops: {'license': ['standard_error', 'excerpt_meta_feature']}\n   INFO ->  feature engineer: Diff\n100%|██████████| 1/1 [00:00<00:00, 23.94it/s]\n   INFO ->  featureDiff ops: {'license': ['standard_error', 'excerpt_meta_feature']}\n   INFO ->  feature engineer: Stat\n100%|██████████| 1/1 [00:00<00:00, 48.08it/s]\n   INFO ->  featureStat ops: {'license': {'id': ['nunique'], 'standard_error': ['mean', 'min', 'max', 'median', 'std'], 'excerpt_meta_feature': ['mean', 'min', 'max', 'median', 'std']}}\n   INFO ->  feature engineer: NLP\n  0%|          | 0/1 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"(1890, 98) (944, 98)\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Training on model 1 - fold 1\nTraining until validation scores don't improve for 150 rounds\n[20]\ttraining's auc: 0.65212\tvalid_1's auc: 0.655716\n[40]\ttraining's auc: 0.65287\tvalid_1's auc: 0.656997\n[60]\ttraining's auc: 0.652993\tvalid_1's auc: 0.657898\n[80]\ttraining's auc: 0.653785\tvalid_1's auc: 0.658942\n[100]\ttraining's auc: 0.65408\tvalid_1's auc: 0.65899\n[120]\ttraining's auc: 0.654514\tvalid_1's auc: 0.659132\n[140]\ttraining's auc: 0.655216\tvalid_1's auc: 0.65899\n[160]\ttraining's auc: 0.663799\tvalid_1's auc: 0.653107\n[180]\ttraining's auc: 0.664395\tvalid_1's auc: 0.655527\n[200]\ttraining's auc: 0.664483\tvalid_1's auc: 0.655384\n[220]\ttraining's auc: 0.664558\tvalid_1's auc: 0.655432\n[240]\ttraining's auc: 0.66461\tvalid_1's auc: 0.655337\n[260]\ttraining's auc: 0.664622\tvalid_1's auc: 0.655337\nEarly stopping, best iteration is:\n[125]\ttraining's auc: 0.655031\tvalid_1's auc: 0.659274\nModel 1 - Fold 1 finished in 0:00:01.407168\nTraining on model 1 - fold 2\nTraining until validation scores don't improve for 150 rounds\n[20]\ttraining's auc: 0.659791\tvalid_1's auc: 0.623916\n[40]\ttraining's auc: 0.659832\tvalid_1's auc: 0.626454\n[60]\ttraining's auc: 0.66025\tvalid_1's auc: 0.625129\n[80]\ttraining's auc: 0.66095\tvalid_1's auc: 0.625893\n[100]\ttraining's auc: 0.661367\tvalid_1's auc: 0.626162\n[120]\ttraining's auc: 0.66162\tvalid_1's auc: 0.626477\n[140]\ttraining's auc: 0.661837\tvalid_1's auc: 0.626432\n[160]\ttraining's auc: 0.663153\tvalid_1's auc: 0.627195\n[180]\ttraining's auc: 0.663186\tvalid_1's auc: 0.62724\n[200]\ttraining's auc: 0.663666\tvalid_1's auc: 0.627554\n[220]\ttraining's auc: 0.664004\tvalid_1's auc: 0.628902\n[240]\ttraining's auc: 0.66386\tvalid_1's auc: 0.627599\n[260]\ttraining's auc: 0.664222\tvalid_1's auc: 0.629396\n[280]\ttraining's auc: 0.664366\tvalid_1's auc: 0.629531\n[300]\ttraining's auc: 0.664357\tvalid_1's auc: 0.629486\n[320]\ttraining's auc: 0.664368\tvalid_1's auc: 0.629576\n[340]\ttraining's auc: 0.667194\tvalid_1's auc: 0.608915\n[360]\ttraining's auc: 0.667215\tvalid_1's auc: 0.608893\n[380]\ttraining's auc: 0.668112\tvalid_1's auc: 0.608893\n[400]\ttraining's auc: 0.668505\tvalid_1's auc: 0.607456\n[420]\ttraining's auc: 0.668528\tvalid_1's auc: 0.607815\n[440]\ttraining's auc: 0.668534\tvalid_1's auc: 0.607815\n[460]\ttraining's auc: 0.66856\tvalid_1's auc: 0.607815\nEarly stopping, best iteration is:\n[314]\ttraining's auc: 0.664374\tvalid_1's auc: 0.629576\nModel 1 - Fold 2 finished in 0:00:02.303236\nTraining on model 1 - fold 3\nTraining until validation scores don't improve for 150 rounds\n[20]\ttraining's auc: 0.657474\tvalid_1's auc: 0.597611\n[40]\ttraining's auc: 0.654698\tvalid_1's auc: 0.644981\n[60]\ttraining's auc: 0.655795\tvalid_1's auc: 0.643249\n[80]\ttraining's auc: 0.656921\tvalid_1's auc: 0.642732\n[100]\ttraining's auc: 0.659891\tvalid_1's auc: 0.596964\n[120]\ttraining's auc: 0.660387\tvalid_1's auc: 0.597016\n[140]\ttraining's auc: 0.661336\tvalid_1's auc: 0.592569\n[160]\ttraining's auc: 0.661485\tvalid_1's auc: 0.592569\n[180]\ttraining's auc: 0.661644\tvalid_1's auc: 0.592517\n[200]\ttraining's auc: 0.661681\tvalid_1's auc: 0.592672\nEarly stopping, best iteration is:\n[51]\ttraining's auc: 0.654873\tvalid_1's auc: 0.645317\nModel 1 - Fold 3 finished in 0:00:01.076738\nTraining on model 1 - fold 4\nTraining until validation scores don't improve for 150 rounds\n[20]\ttraining's auc: 0.645639\tvalid_1's auc: 0.673941\n[40]\ttraining's auc: 0.646728\tvalid_1's auc: 0.675479\n[60]\ttraining's auc: 0.64748\tvalid_1's auc: 0.675022\n[80]\ttraining's auc: 0.648846\tvalid_1's auc: 0.674544\n[100]\ttraining's auc: 0.648966\tvalid_1's auc: 0.673962\n[120]\ttraining's auc: 0.649144\tvalid_1's auc: 0.673297\n[140]\ttraining's auc: 0.649662\tvalid_1's auc: 0.673962\n[160]\ttraining's auc: 0.649743\tvalid_1's auc: 0.673962\nEarly stopping, best iteration is:\n[28]\ttraining's auc: 0.646251\tvalid_1's auc: 0.676269\nModel 1 - Fold 4 finished in 0:00:00.928579\nTraining on model 1 - fold 5\nTraining until validation scores don't improve for 150 rounds\n[20]\ttraining's auc: 0.652136\tvalid_1's auc: 0.649985\n[40]\ttraining's auc: 0.653381\tvalid_1's auc: 0.652316\n[60]\ttraining's auc: 0.654091\tvalid_1's auc: 0.651102\n[80]\ttraining's auc: 0.654416\tvalid_1's auc: 0.650277\n[100]\ttraining's auc: 0.654551\tvalid_1's auc: 0.650908\n[120]\ttraining's auc: 0.654773\tvalid_1's auc: 0.651054\n[140]\ttraining's auc: 0.655217\tvalid_1's auc: 0.651442\n[160]\ttraining's auc: 0.65593\tvalid_1's auc: 0.653433\n[180]\ttraining's auc: 0.662185\tvalid_1's auc: 0.658678\n[200]\ttraining's auc: 0.662081\tvalid_1's auc: 0.659164\n[220]\ttraining's auc: 0.662395\tvalid_1's auc: 0.658921\n[240]\ttraining's auc: 0.662395\tvalid_1's auc: 0.658921\n[260]\ttraining's auc: 0.662455\tvalid_1's auc: 0.658921\n[280]\ttraining's auc: 0.662737\tvalid_1's auc: 0.658921\n[300]\ttraining's auc: 0.662737\tvalid_1's auc: 0.658921\n[320]\ttraining's auc: 0.662803\tvalid_1's auc: 0.658921\n[340]\ttraining's auc: 0.662812\tvalid_1's auc: 0.658921\n[360]\ttraining's auc: 0.662863\tvalid_1's auc: 0.660815\n[380]\ttraining's auc: 0.663122\tvalid_1's auc: 0.660815\n[400]\ttraining's auc: 0.663108\tvalid_1's auc: 0.660815\n[420]\ttraining's auc: 0.663082\tvalid_1's auc: 0.660815\n[440]\ttraining's auc: 0.663128\tvalid_1's auc: 0.660815\n[460]\ttraining's auc: 0.663128\tvalid_1's auc: 0.660815\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:09<00:00,  9.18s/it]\u001b[A\n100%|██████████| 1/1 [00:09<00:00,  9.22s/it]\n   INFO ->  featureNlp ops: [['url_legal']]\n   INFO ->  feature engineer: Count\n","output_type":"stream"},{"name":"stdout","text":"[480]\ttraining's auc: 0.663145\tvalid_1's auc: 0.660815\n[500]\ttraining's auc: 0.66318\tvalid_1's auc: 0.660815\nDid not meet early stopping. Best iteration is:\n[386]\ttraining's auc: 0.66326\tvalid_1's auc: 0.660184\nModel 1 - Fold 5 finished in 0:00:03.460491\ndone!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 174.27it/s]\n   INFO ->  featureCount ops: [['license']]\n   INFO ->  feature engineer: Rank\n100%|██████████| 1/1 [00:00<00:00, 137.44it/s]\n   INFO ->  featureRank ops: {'license': {'target': ['rank'], 'standard_error': ['rank'], 'excerpt_meta_feature': ['rank']}}\n   INFO ->  ignore image feature\n100%|██████████| 6/6 [00:00<00:00, 671.43it/s]\n   INFO ->  label_encoder_list: ['id', 'license']\n   INFO ->  ordinal_encoder_list: []\n   INFO ->  feature combination\n100%|██████████| 9/9 [00:00<00:00, 1451.54it/s]\n   INFO ->  shape of FE_all: (2834, 70), shape of train: (1890, 70), shape of test: (944, 70)\n   INFO ->  feature filter\n100%|██████████| 67/67 [00:00<00:00, 720.34it/s]\n   INFO ->  filtered features: ['target', 'license__target__rank']\n   INFO ->  used_features: ['standard_error', 'excerpt_meta_feature', 'url_legal_nlp', 'COUNT_license', 'license__id__nunique', 'license__standard_error__mean', 'license__standard_error__min', 'license__standard_error__max', 'license__standard_error__median', 'license__standard_error__std', 'license__excerpt_meta_feature__mean', 'license__excerpt_meta_feature__min', 'license__excerpt_meta_feature__max', 'license__excerpt_meta_feature__median', 'license__excerpt_meta_feature__std', 'license__standard_error__rank', 'license__excerpt_meta_feature__rank', 'license__standard_error__shift__-30', 'license__standard_error__shift__-24', 'license__standard_error__shift__-7', 'license__standard_error__shift__-3', 'license__standard_error__shift__-2', 'license__standard_error__shift__-1', 'license__standard_error__shift__1', 'license__standard_error__shift__2', 'license__standard_error__shift__3', 'license__standard_error__shift__7', 'license__standard_error__shift__24', 'license__standard_error__shift__30', 'license__excerpt_meta_feature__shift__-30', 'license__excerpt_meta_feature__shift__-24', 'license__excerpt_meta_feature__shift__-7', 'license__excerpt_meta_feature__shift__-3', 'license__excerpt_meta_feature__shift__-2', 'license__excerpt_meta_feature__shift__-1', 'license__excerpt_meta_feature__shift__1', 'license__excerpt_meta_feature__shift__2', 'license__excerpt_meta_feature__shift__3', 'license__excerpt_meta_feature__shift__7', 'license__excerpt_meta_feature__shift__24', 'license__excerpt_meta_feature__shift__30', 'license__standard_error__diff__-30', 'license__standard_error__diff__-24', 'license__standard_error__diff__-7', 'license__standard_error__diff__-3', 'license__standard_error__diff__-2', 'license__standard_error__diff__-1', 'license__standard_error__diff__1', 'license__standard_error__diff__2', 'license__standard_error__diff__3', 'license__standard_error__diff__7', 'license__standard_error__diff__24', 'license__standard_error__diff__30', 'license__excerpt_meta_feature__diff__-30', 'license__excerpt_meta_feature__diff__-24', 'license__excerpt_meta_feature__diff__-7', 'license__excerpt_meta_feature__diff__-3', 'license__excerpt_meta_feature__diff__-2', 'license__excerpt_meta_feature__diff__-1', 'license__excerpt_meta_feature__diff__1', 'license__excerpt_meta_feature__diff__2', 'license__excerpt_meta_feature__diff__3', 'license__excerpt_meta_feature__diff__7', 'license__excerpt_meta_feature__diff__24', 'license__excerpt_meta_feature__diff__30']\n   INFO ->  start training lightgbm model\n   INFO ->  (1890, 65)\n","output_type":"stream"},{"name":"stdout","text":"Training on fold 1\nTraining until validation scores don't improve for 150 rounds\n[100]\ttraining's rmse: 0.632409\tvalid_1's rmse: 0.66319\n[200]\ttraining's rmse: 0.522142\tvalid_1's rmse: 0.603235\n[300]\ttraining's rmse: 0.488352\tvalid_1's rmse: 0.596521\n[400]\ttraining's rmse: 0.472772\tvalid_1's rmse: 0.597198\nEarly stopping, best iteration is:\n[291]\ttraining's rmse: 0.490233\tvalid_1's rmse: 0.596443\nMSE: 0.35574478133300824\nFold 1 finished in 0:00:05.787609\nTraining on fold 2\nTraining until validation scores don't improve for 150 rounds\n[100]\ttraining's rmse: 0.632321\tvalid_1's rmse: 0.674792\n[200]\ttraining's rmse: 0.529629\tvalid_1's rmse: 0.580651\n[300]\ttraining's rmse: 0.497538\tvalid_1's rmse: 0.560305\n[400]\ttraining's rmse: 0.48264\tvalid_1's rmse: 0.555218\n[500]\ttraining's rmse: 0.474282\tvalid_1's rmse: 0.554382\n[600]\ttraining's rmse: 0.467293\tvalid_1's rmse: 0.554919\nEarly stopping, best iteration is:\n[487]\ttraining's rmse: 0.47509\tvalid_1's rmse: 0.554308\nMSE: 0.307257663142463\nFold 2 finished in 0:00:08.855160\nTraining on fold 3\nTraining until validation scores don't improve for 150 rounds\n[100]\ttraining's rmse: 0.626276\tvalid_1's rmse: 0.722031\n[200]\ttraining's rmse: 0.520253\tvalid_1's rmse: 0.661718\n[300]\ttraining's rmse: 0.488604\tvalid_1's rmse: 0.647148\n[400]\ttraining's rmse: 0.474663\tvalid_1's rmse: 0.644567\n[500]\ttraining's rmse: 0.466813\tvalid_1's rmse: 0.644827\nEarly stopping, best iteration is:\n[394]\ttraining's rmse: 0.475176\tvalid_1's rmse: 0.644396\nMSE: 0.41524614254762704\nFold 3 finished in 0:00:07.738986\nTraining on fold 4\nTraining until validation scores don't improve for 150 rounds\n[100]\ttraining's rmse: 0.62876\tvalid_1's rmse: 0.681513\n[200]\ttraining's rmse: 0.521398\tvalid_1's rmse: 0.62056\n[300]\ttraining's rmse: 0.488773\tvalid_1's rmse: 0.61103\n[400]\ttraining's rmse: 0.473803\tvalid_1's rmse: 0.610416\n[500]\ttraining's rmse: 0.465279\tvalid_1's rmse: 0.610099\n[600]\ttraining's rmse: 0.459188\tvalid_1's rmse: 0.61022\nEarly stopping, best iteration is:\n[541]\ttraining's rmse: 0.462677\tvalid_1's rmse: 0.609874\nMSE: 0.3719457271882805\nFold 4 finished in 0:00:09.157437\nTraining on fold 5\nTraining until validation scores don't improve for 150 rounds\n[100]\ttraining's rmse: 0.642628\tvalid_1's rmse: 0.633705\n[200]\ttraining's rmse: 0.539495\tvalid_1's rmse: 0.548637\n[300]\ttraining's rmse: 0.506616\tvalid_1's rmse: 0.532149\n[400]\ttraining's rmse: 0.490542\tvalid_1's rmse: 0.529058\n[500]\ttraining's rmse: 0.480716\tvalid_1's rmse: 0.528785\n","output_type":"stream"},{"name":"stderr","text":"   INFO ->  feature importance\n   INFO ->                                    feature  fold_1  fold_2  fold_3  fold_4  \\\n0                          standard_error    1328    1545    1459    1594   \n1                    excerpt_meta_feature    1098    1460    1293    1401   \n2     license__excerpt_meta_feature__rank     680     770     716     799   \n3           license__standard_error__rank     503     672     646     760   \n4                           url_legal_nlp     237     316     272     379   \n..                                    ...     ...     ...     ...     ...   \n60     license__excerpt_meta_feature__max       0       4       5       6   \n61     license__excerpt_meta_feature__std       0       2       2       1   \n62           license__standard_error__std       0       1       2       1   \n63     license__excerpt_meta_feature__min       0       1       0       2   \n64  license__excerpt_meta_feature__median       0       0       0       2   \n\n    fold_5  average  \n0     1502   1485.6  \n1     1423   1335.0  \n2      761    745.2  \n3      693    654.8  \n4      277    296.2  \n..     ...      ...  \n60       4      3.8  \n61       4      1.8  \n62       3      1.4  \n63       4      1.4  \n64       4      1.2  \n\n[65 rows x 7 columns]\n   INFO ->  start training xgboost model\n   INFO ->  (1890, 65)\n","output_type":"stream"},{"name":"stdout","text":"Early stopping, best iteration is:\n[432]\ttraining's rmse: 0.48697\tvalid_1's rmse: 0.528675\nMSE: 0.279497485857031\nFold 5 finished in 0:00:07.519047\nTraining on fold 1\n[13:59:52] WARNING: ../src/learner.cc:627: \nParameters: { \"verbose_eval\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\tvalidation_0-rmse:1.77590\n[100]\tvalidation_0-rmse:0.96613\n[200]\tvalidation_0-rmse:0.71288\n[300]\tvalidation_0-rmse:0.63743\n[400]\tvalidation_0-rmse:0.61348\n[500]\tvalidation_0-rmse:0.60279\n[600]\tvalidation_0-rmse:0.59838\n[700]\tvalidation_0-rmse:0.59572\n[800]\tvalidation_0-rmse:0.59491\n[900]\tvalidation_0-rmse:0.59451\n[1000]\tvalidation_0-rmse:0.59424\n[1100]\tvalidation_0-rmse:0.59413\n[1200]\tvalidation_0-rmse:0.59366\n[1300]\tvalidation_0-rmse:0.59357\n[1400]\tvalidation_0-rmse:0.59356\n[1500]\tvalidation_0-rmse:0.59339\n[1600]\tvalidation_0-rmse:0.59332\n[1608]\tvalidation_0-rmse:0.59333\nMSE: 0.35200777649879456\nFold 1 finished in 0:00:07.337860\nTraining on fold 2\n[13:59:58] WARNING: ../src/learner.cc:627: \nParameters: { \"verbose_eval\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\tvalidation_0-rmse:1.79224\n[100]\tvalidation_0-rmse:1.01744\n[200]\tvalidation_0-rmse:0.78761\n[300]\tvalidation_0-rmse:0.71584\n[400]\tvalidation_0-rmse:0.68798\n[500]\tvalidation_0-rmse:0.67635\n[600]\tvalidation_0-rmse:0.67046\n[700]\tvalidation_0-rmse:0.66798\n[800]\tvalidation_0-rmse:0.66680\n[900]\tvalidation_0-rmse:0.66626\n[1000]\tvalidation_0-rmse:0.66556\n[1100]\tvalidation_0-rmse:0.66495\n[1200]\tvalidation_0-rmse:0.66477\n[1300]\tvalidation_0-rmse:0.66460\n[1400]\tvalidation_0-rmse:0.66439\n[1500]\tvalidation_0-rmse:0.66429\n[1600]\tvalidation_0-rmse:0.66420\n[1700]\tvalidation_0-rmse:0.66405\n[1800]\tvalidation_0-rmse:0.66389\n[1900]\tvalidation_0-rmse:0.66389\n[1902]\tvalidation_0-rmse:0.66389\nMSE: 0.44074690341949463\nFold 2 finished in 0:00:06.321360\nTraining on fold 3\n[14:00:04] WARNING: ../src/learner.cc:627: \nParameters: { \"verbose_eval\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\tvalidation_0-rmse:1.72616\n[100]\tvalidation_0-rmse:0.91065\n[200]\tvalidation_0-rmse:0.67576\n[300]\tvalidation_0-rmse:0.60604\n[400]\tvalidation_0-rmse:0.58404\n[500]\tvalidation_0-rmse:0.57575\n[600]\tvalidation_0-rmse:0.57343\n[700]\tvalidation_0-rmse:0.57244\n[800]\tvalidation_0-rmse:0.57219\n[883]\tvalidation_0-rmse:0.57225\nMSE: 0.32737332582473755\nFold 3 finished in 0:00:04.808808\nTraining on fold 4\n[14:00:09] WARNING: ../src/learner.cc:627: \nParameters: { \"verbose_eval\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\tvalidation_0-rmse:1.67414\n[100]\tvalidation_0-rmse:0.91457\n[200]\tvalidation_0-rmse:0.71460\n[300]\tvalidation_0-rmse:0.66236\n[400]\tvalidation_0-rmse:0.64736\n[500]\tvalidation_0-rmse:0.64198\n[600]\tvalidation_0-rmse:0.64065\n[700]\tvalidation_0-rmse:0.63964\n[800]\tvalidation_0-rmse:0.63921\n[900]\tvalidation_0-rmse:0.63884\n[1000]\tvalidation_0-rmse:0.63873\n[1100]\tvalidation_0-rmse:0.63875\n[1200]\tvalidation_0-rmse:0.63858\n[1300]\tvalidation_0-rmse:0.63838\n[1400]\tvalidation_0-rmse:0.63809\n[1500]\tvalidation_0-rmse:0.63806\n[1600]\tvalidation_0-rmse:0.63801\n[1700]\tvalidation_0-rmse:0.63801\n[1800]\tvalidation_0-rmse:0.63789\n[1900]\tvalidation_0-rmse:0.63783\n[2000]\tvalidation_0-rmse:0.63782\n[2100]\tvalidation_0-rmse:0.63779\n[2200]\tvalidation_0-rmse:0.63777\n[2300]\tvalidation_0-rmse:0.63779\n[2319]\tvalidation_0-rmse:0.63778\nMSE: 0.4067358672618866\nFold 4 finished in 0:00:08.618461\nTraining on fold 5\n[14:00:18] WARNING: ../src/learner.cc:627: \nParameters: { \"verbose_eval\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\tvalidation_0-rmse:1.86170\n[100]\tvalidation_0-rmse:1.04734\n[200]\tvalidation_0-rmse:0.79613\n[300]\tvalidation_0-rmse:0.71496\n[400]\tvalidation_0-rmse:0.68362\n[500]\tvalidation_0-rmse:0.67002\n[600]\tvalidation_0-rmse:0.66285\n[700]\tvalidation_0-rmse:0.65960\n[800]\tvalidation_0-rmse:0.65829\n[900]\tvalidation_0-rmse:0.65717\n[1000]\tvalidation_0-rmse:0.65673\n[1100]\tvalidation_0-rmse:0.65629\n[1200]\tvalidation_0-rmse:0.65599\n[1300]\tvalidation_0-rmse:0.65583\n[1400]\tvalidation_0-rmse:0.65567\n[1500]\tvalidation_0-rmse:0.65548\n[1600]\tvalidation_0-rmse:0.65537\n[1700]\tvalidation_0-rmse:0.65523\n[1800]\tvalidation_0-rmse:0.65515\n[1900]\tvalidation_0-rmse:0.65507\n[2000]\tvalidation_0-rmse:0.65488\n[2100]\tvalidation_0-rmse:0.65482\n[2174]\tvalidation_0-rmse:0.65482\nMSE: 0.42878836393356323\nFold 5 finished in 0:00:06.992971\nTraining on fold 6\n[14:00:25] WARNING: ../src/learner.cc:627: \nParameters: { \"verbose_eval\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\tvalidation_0-rmse:1.75944\n[100]\tvalidation_0-rmse:0.94337\n[200]\tvalidation_0-rmse:0.70999\n[300]\tvalidation_0-rmse:0.64248\n[400]\tvalidation_0-rmse:0.62090\n[500]\tvalidation_0-rmse:0.61182\n[600]\tvalidation_0-rmse:0.60833\n[700]\tvalidation_0-rmse:0.60710\n[800]\tvalidation_0-rmse:0.60627\n[900]\tvalidation_0-rmse:0.60612\n[1000]\tvalidation_0-rmse:0.60595\n[1100]\tvalidation_0-rmse:0.60583\n[1200]\tvalidation_0-rmse:0.60569\n[1300]\tvalidation_0-rmse:0.60551\n[1400]\tvalidation_0-rmse:0.60539\n[1500]\tvalidation_0-rmse:0.60522\n[1593]\tvalidation_0-rmse:0.60522\nMSE: 0.36628788709640503\nFold 6 finished in 0:00:06.173442\nTraining on fold 7\n[14:00:31] WARNING: ../src/learner.cc:627: \nParameters: { \"verbose_eval\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\tvalidation_0-rmse:1.87368\n[100]\tvalidation_0-rmse:1.03955\n[200]\tvalidation_0-rmse:0.75943\n[300]\tvalidation_0-rmse:0.66062\n[400]\tvalidation_0-rmse:0.62334\n[500]\tvalidation_0-rmse:0.60704\n[600]\tvalidation_0-rmse:0.59932\n[700]\tvalidation_0-rmse:0.59549\n[800]\tvalidation_0-rmse:0.59365\n[900]\tvalidation_0-rmse:0.59203\n[1000]\tvalidation_0-rmse:0.59122\n[1100]\tvalidation_0-rmse:0.59052\n[1200]\tvalidation_0-rmse:0.58999\n[1300]\tvalidation_0-rmse:0.58979\n[1400]\tvalidation_0-rmse:0.58938\n[1500]\tvalidation_0-rmse:0.58878\n[1600]\tvalidation_0-rmse:0.58860\n[1700]\tvalidation_0-rmse:0.58852\n[1800]\tvalidation_0-rmse:0.58831\n[1900]\tvalidation_0-rmse:0.58812\n[2000]\tvalidation_0-rmse:0.58788\n[2100]\tvalidation_0-rmse:0.58782\n[2200]\tvalidation_0-rmse:0.58767\n[2300]\tvalidation_0-rmse:0.58760\n[2400]\tvalidation_0-rmse:0.58743\n[2500]\tvalidation_0-rmse:0.58723\n[2600]\tvalidation_0-rmse:0.58708\n[2647]\tvalidation_0-rmse:0.58704\nMSE: 0.34461432695388794\nFold 7 finished in 0:00:08.132093\nTraining on fold 8\n[14:00:39] WARNING: ../src/learner.cc:627: \nParameters: { \"verbose_eval\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\tvalidation_0-rmse:1.82484\n[100]\tvalidation_0-rmse:1.01617\n[200]\tvalidation_0-rmse:0.76725\n[300]\tvalidation_0-rmse:0.68286\n[400]\tvalidation_0-rmse:0.65498\n[500]\tvalidation_0-rmse:0.64464\n[600]\tvalidation_0-rmse:0.63934\n[700]\tvalidation_0-rmse:0.63633\n[800]\tvalidation_0-rmse:0.63479\n[900]\tvalidation_0-rmse:0.63374\n[1000]\tvalidation_0-rmse:0.63344\n[1100]\tvalidation_0-rmse:0.63302\n[1200]\tvalidation_0-rmse:0.63272\n[1300]\tvalidation_0-rmse:0.63249\n[1400]\tvalidation_0-rmse:0.63235\n[1500]\tvalidation_0-rmse:0.63206\n[1600]\tvalidation_0-rmse:0.63195\n[1700]\tvalidation_0-rmse:0.63187\n[1800]\tvalidation_0-rmse:0.63183\n[1825]\tvalidation_0-rmse:0.63183\nMSE: 0.39920827746391296\nFold 8 finished in 0:00:06.036265\nTraining on fold 9\n[14:00:45] WARNING: ../src/learner.cc:627: \nParameters: { \"verbose_eval\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\tvalidation_0-rmse:1.73686\n[100]\tvalidation_0-rmse:0.92563\n[200]\tvalidation_0-rmse:0.69846\n[300]\tvalidation_0-rmse:0.63623\n[400]\tvalidation_0-rmse:0.61804\n[500]\tvalidation_0-rmse:0.61360\n[600]\tvalidation_0-rmse:0.61212\n[700]\tvalidation_0-rmse:0.61160\n[800]\tvalidation_0-rmse:0.61106\n[900]\tvalidation_0-rmse:0.61090\n[1000]\tvalidation_0-rmse:0.61087\n[1010]\tvalidation_0-rmse:0.61088\nMSE: 0.373142272233963\nFold 9 finished in 0:00:06.012106\nTraining on fold 10\n[14:00:51] WARNING: ../src/learner.cc:627: \nParameters: { \"verbose_eval\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\tvalidation_0-rmse:1.81791\n[100]\tvalidation_0-rmse:0.97902\n[200]\tvalidation_0-rmse:0.72517\n[300]\tvalidation_0-rmse:0.64566\n[400]\tvalidation_0-rmse:0.61887\n[500]\tvalidation_0-rmse:0.60739\n[600]\tvalidation_0-rmse:0.60185\n[700]\tvalidation_0-rmse:0.59968\n[800]\tvalidation_0-rmse:0.59878\n[900]\tvalidation_0-rmse:0.59838\n[1000]\tvalidation_0-rmse:0.59782\n[1100]\tvalidation_0-rmse:0.59768\n[1200]\tvalidation_0-rmse:0.59733\n[1300]\tvalidation_0-rmse:0.59695\n[1400]\tvalidation_0-rmse:0.59670\n[1500]\tvalidation_0-rmse:0.59659\n[1600]\tvalidation_0-rmse:0.59634\n[1700]\tvalidation_0-rmse:0.59619\n[1800]\tvalidation_0-rmse:0.59613\n[1822]\tvalidation_0-rmse:0.59613\n","output_type":"stream"},{"name":"stderr","text":"   INFO ->  Average KFold RMSE: 0.37942707538604736\n","output_type":"stream"},{"name":"stdout","text":"MSE: 0.3553660213947296\nFold 10 finished in 0:00:06.307361\n","output_type":"stream"}]},{"cell_type":"code","source":"val = pd.read_csv(f'sub_val.csv')\nfrom sklearn.metrics import mean_squared_error\nRMSE = np.sqrt(mean_squared_error(val['target'], sub['target']))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T14:00:58.261507Z","iopub.execute_input":"2022-05-08T14:00:58.263315Z","iopub.status.idle":"2022-05-08T14:00:58.284480Z","shell.execute_reply.started":"2022-05-08T14:00:58.263269Z","shell.execute_reply":"2022-05-08T14:00:58.283774Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"RMSE","metadata":{"execution":{"iopub.status.busy":"2022-05-08T14:00:58.285711Z","iopub.execute_input":"2022-05-08T14:00:58.285976Z","iopub.status.idle":"2022-05-08T14:00:58.292566Z","shell.execute_reply.started":"2022-05-08T14:00:58.285942Z","shell.execute_reply":"2022-05-08T14:00:58.291882Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0.5970512889627345"},"metadata":{}}]}]}