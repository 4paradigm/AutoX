{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import KFold\n\ntrain = pd.read_csv('../input/titanic/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:38:16.466535Z","iopub.execute_input":"2022-05-08T15:38:16.466853Z","iopub.status.idle":"2022-05-08T15:38:17.505378Z","shell.execute_reply.started":"2022-05-08T15:38:16.466770Z","shell.execute_reply":"2022-05-08T15:38:17.504594Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=3, random_state=1001,shuffle=True)\nfor i, (train_index, val_index) in enumerate(kf.split(train)):\n    trn= train.iloc[train_index].reset_index()\n    val= train.iloc[val_index].reset_index()\n    \ntrn = trn.drop(columns=['index'])\nval = val.drop(columns=['index'])\n\nval.to_csv('sub_val.csv',index=False)\ntrn.to_csv('sub_train.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:38:18.504783Z","iopub.execute_input":"2022-05-08T15:38:18.505307Z","iopub.status.idle":"2022-05-08T15:38:18.538750Z","shell.execute_reply.started":"2022-05-08T15:38:18.505264Z","shell.execute_reply":"2022-05-08T15:38:18.537745Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# !rm -r ./autox\n!git clone https://github.com/4paradigm/autox.git\n!pip install ./autox","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-08T15:38:21.385279Z","iopub.execute_input":"2022-05-08T15:38:21.385545Z","iopub.status.idle":"2022-05-08T15:39:01.389679Z","shell.execute_reply.started":"2022-05-08T15:38:21.385509Z","shell.execute_reply":"2022-05-08T15:39:01.388769Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'autox'...\nremote: Enumerating objects: 2699, done.\u001b[K\nremote: Counting objects: 100% (612/612), done.\u001b[K\nremote: Compressing objects: 100% (153/153), done.\u001b[K\nremote: Total 2699 (delta 479), reused 570 (delta 452), pack-reused 2087\u001b[K\nReceiving objects: 100% (2699/2699), 11.87 MiB | 18.85 MiB/s, done.\nResolving deltas: 100% (1723/1723), done.\nProcessing ./autox\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: lightgbm in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (3.3.1)\nRequirement already satisfied: xgboost in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (1.6.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (1.9.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (1.21.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (1.3.5)\nRequirement already satisfied: sklearn in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (4.63.0)\nRequirement already satisfied: optuna in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (2.10.0)\nCollecting img2vec_pytorch\n  Downloading img2vec_pytorch-1.0.1-py3-none-any.whl (6.9 kB)\nCollecting pypinyin\n  Downloading pypinyin-0.46.0-py2.py3-none-any.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hRequirement already satisfied: keras in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (2.6.0)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (2.6.3)\nRequirement already satisfied: gensim in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (4.0.1)\nCollecting glove-python-binary\n  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.9/948.9 KB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (4.18.0)\nCollecting datasets\n  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.4/325.4 KB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (3.8.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (2.27.1)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (2022.3.0)\nCollecting responses<0.19\n  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (7.0.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (4.11.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (0.3.4)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (0.70.12.2)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (0.5.1)\nCollecting xxhash\n  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (21.3)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim->automl-x==0.3.0) (5.2.1)\nRequirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim->automl-x==0.3.0) (1.7.3)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from img2vec_pytorch->automl-x==0.3.0) (0.10.1)\nRequirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.7/site-packages (from lightgbm->automl-x==0.3.0) (1.0.2)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from lightgbm->automl-x==0.3.0) (0.37.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (6.0)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (6.6.0)\nRequirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (1.4.32)\nRequirement already satisfied: cliff in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (3.10.1)\nRequirement already satisfied: cmaes>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (0.8.2)\nRequirement already satisfied: alembic in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (1.7.7)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->automl-x==0.3.0) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->automl-x==0.3.0) (2021.3)\nRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (2.6.0)\nCollecting six~=1.15.0\n  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (1.43.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (1.6.3)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (5.0)\nCollecting absl-py~=0.10\n  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (3.19.4)\nRequirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (3.1.0)\nCollecting wrapt~=1.12.1\n  Downloading wrapt-1.12.1.tar.gz (27 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting numpy\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (0.4.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (1.1.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (1.12)\nCollecting typing-extensions<3.11,>=3.7\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (1.1.2)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (3.3.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (0.2.0)\nRequirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (2.6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers->automl-x==0.3.0) (2021.11.10)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers->automl-x==0.3.0) (0.12.1)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers->automl-x==0.3.0) (0.0.49)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers->automl-x==0.3.0) (3.6.0)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow->automl-x==0.3.0) (1.5.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets->automl-x==0.3.0) (3.0.7)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->automl-x==0.3.0) (2.0.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->automl-x==0.3.0) (2021.10.8)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->automl-x==0.3.0) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->automl-x==0.3.0) (1.26.8)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm->automl-x==0.3.0) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm->automl-x==0.3.0) (1.0.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna->automl-x==0.3.0) (1.1.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (3.3.6)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (0.6.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (1.8.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (2.0.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (59.8.0)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (1.35.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (0.4.6)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (21.4.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (4.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (1.2.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (0.13.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (1.7.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (6.0.2)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from alembic->optuna->automl-x==0.3.0) (5.4.0)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic->optuna->automl-x==0.3.0) (1.2.0)\nRequirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna->automl-x==0.3.0) (5.8.1)\nRequirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna->automl-x==0.3.0) (3.2.0)\nRequirement already satisfied: autopage>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna->automl-x==0.3.0) (0.5.0)\nRequirement already satisfied: cmd2>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna->automl-x==0.3.0) (2.4.1)\nRequirement already satisfied: stevedore>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna->automl-x==0.3.0) (3.5.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets->automl-x==0.3.0) (3.7.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers->automl-x==0.3.0) (8.0.4)\nRequirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->img2vec_pytorch->automl-x==0.3.0) (9.0.1)\nRequirement already satisfied: pyperclip>=1.6 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna->automl-x==0.3.0) (1.8.2)\nRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna->automl-x==0.3.0) (0.2.5)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (4.8)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (1.3.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic->optuna->automl-x==0.3.0) (2.0.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (3.2.0)\nBuilding wheels for collected packages: automl-x, wrapt\n  Building wheel for automl-x (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for automl-x: filename=automl_x-0.3.0-py3-none-any.whl size=4809171 sha256=be44c0ac63ef4bb8976fae3b33f6fb97ad64e9c9b6262621ebf84331255f2161\n  Stored in directory: /tmp/pip-ephem-wheel-cache-s7fs_ekk/wheels/d8/d0/3f/17c06e69ec61f9c250a3274a013cc8a24b47965ec33f40c73d\n  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=77051 sha256=cd9349d409dbb8330a60352a115cc3672ee74ccb7bdef3cf8e875ca3c79b68bc\n  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\nSuccessfully built automl-x wrapt\nInstalling collected packages: wrapt, typing-extensions, xxhash, six, pypinyin, numpy, responses, absl-py, img2vec_pytorch, glove-python-binary, datasets, automl-x\n  Attempting uninstall: wrapt\n    Found existing installation: wrapt 1.14.0\n    Uninstalling wrapt-1.14.0:\n      Successfully uninstalled wrapt-1.14.0\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.2.0\n    Uninstalling typing_extensions-4.2.0:\n      Successfully uninstalled typing_extensions-4.2.0\n  Attempting uninstall: six\n    Found existing installation: six 1.16.0\n    Uninstalling six-1.16.0:\n      Successfully uninstalled six-1.16.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: absl-py\n    Found existing installation: absl-py 1.0.0\n    Uninstalling absl-py-1.0.0:\n      Successfully uninstalled absl-py-1.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\ntfx-bsl 1.7.0 requires pyarrow<6,>=1, but you have pyarrow 7.0.0 which is incompatible.\ntfx-bsl 1.7.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5, but you have tensorflow 2.6.3 which is incompatible.\ntensorflow-transform 1.7.0 requires pyarrow<6,>=1, but you have pyarrow 7.0.0 which is incompatible.\ntensorflow-transform 1.7.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<2.9,>=1.15.5, but you have tensorflow 2.6.3 which is incompatible.\ntensorflow-serving-api 2.8.0 requires tensorflow<3,>=2.8.0, but you have tensorflow 2.6.3 which is incompatible.\nrich 12.2.0 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytorch-lightning 1.6.1 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npytools 2022.1.5 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.1 which is incompatible.\nimageio 2.16.1 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\ngrpcio-status 1.44.0 requires grpcio>=1.44.0, but you have grpcio 1.43.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-cloud-storage<2.0.0dev,>=1.26.0, but you have google-cloud-storage 2.1.0 which is incompatible.\ngcsfs 2022.2.0 requires fsspec==2022.02.0, but you have fsspec 2022.3.0 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.11.3 which is incompatible.\nfeaturetools 1.8.0 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\napache-beam 2.37.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\napache-beam 2.37.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.4 which is incompatible.\napache-beam 2.37.0 requires pyarrow<7.0.0,>=0.15.1, but you have pyarrow 7.0.0 which is incompatible.\naioitertools 0.10.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.2.0 requires botocore<1.24.22,>=1.24.21, but you have botocore 1.25.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed absl-py-0.15.0 automl-x-0.3.0 datasets-2.1.0 glove-python-binary-0.2.0 img2vec_pytorch-1.0.1 numpy-1.19.5 pypinyin-0.46.0 responses-0.18.0 six-1.15.0 typing-extensions-3.10.0.2 wrapt-1.12.1 xxhash-3.0.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from autox.autox_nlp import NLP_feature\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:39:01.392960Z","iopub.execute_input":"2022-05-08T15:39:01.393275Z","iopub.status.idle":"2022-05-08T15:39:12.497276Z","shell.execute_reply.started":"2022-05-08T15:39:01.393242Z","shell.execute_reply":"2022-05-08T15:39:12.496392Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"df_train = pd.read_csv('sub_train.csv')\ndf_test = pd.read_csv('sub_val.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:39:12.498640Z","iopub.execute_input":"2022-05-08T15:39:12.498898Z","iopub.status.idle":"2022-05-08T15:39:12.515873Z","shell.execute_reply.started":"2022-05-08T15:39:12.498862Z","shell.execute_reply":"2022-05-08T15:39:12.515168Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"use_Toknizer=True\nemb_mode = 'Bert'# TFIDF / Word2Vec / Glove / FastText / Bert\nencode_mode = 'supervise' # unsupervise / supervise\ntext_columns_name = ['Name']\ntarget_column = df_train['Survived']\ncandidate_labels=None","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:39:12.518009Z","iopub.execute_input":"2022-05-08T15:39:12.518379Z","iopub.status.idle":"2022-05-08T15:39:12.526785Z","shell.execute_reply.started":"2022-05-08T15:39:12.518341Z","shell.execute_reply":"2022-05-08T15:39:12.525927Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"nlp = NLP_feature()\n# nlp.do_mlm = True\n# nlp.mlm_epochs=3\n# nlp.model_name = 'microsoft/deberta-v3-base'\nnlp.emb_size=100\nnlp.n_clusters=20\ndf = nlp.fit(df_train,\n             text_columns_name,\n             use_Toknizer,\n             emb_mode,\n             encode_mode,\n             target_column,\n             candidate_labels)","metadata":{"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-05-08T15:39:12.528136Z","iopub.execute_input":"2022-05-08T15:39:12.528487Z","iopub.status.idle":"2022-05-08T15:39:32.147656Z","shell.execute_reply.started":"2022-05-08T15:39:12.528445Z","shell.execute_reply":"2022-05-08T15:39:32.146907Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Fitting column: Name tokenizer\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/285 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98cdab3e01db44e580737ee7b48aef2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2617b498f4e8462cbd58323c1b0881ba"}},"metadata":{}},{"name":"stdout","text":"Fitting column: Name bert embedding\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/16.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51d3b7ce23c04cd2a24ab9d45bda6d4b"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSaving model checkpoint to ./Name_transformer\nConfiguration saved in ./Name_transformer/config.json\nModel weights saved in ./Name_transformer/pytorch_model.bin\nloading configuration file ./Name_transformer/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"./Name_transformer\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file ./Name_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./Name_transformer were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at ./Name_transformer and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Fitting column: Name encoder\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file ./Name_transformer/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"./Name_transformer\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file ./Name_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./Name_transformer were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at ./Name_transformer and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nloading configuration file ./Name_transformer/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"./Name_transformer\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file ./Name_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./Name_transformer were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at ./Name_transformer and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nloading configuration file ./Name_transformer/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"./Name_transformer\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file ./Name_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./Name_transformer were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at ./Name_transformer and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nloading configuration file ./Name_transformer/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"./Name_transformer\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file ./Name_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./Name_transformer were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at ./Name_transformer and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"for column in df.columns:\n    df_train[column] = df[column]\ndf_train = df_train.drop(columns=text_columns_name)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:39:32.149002Z","iopub.execute_input":"2022-05-08T15:39:32.149264Z","iopub.status.idle":"2022-05-08T15:39:32.157757Z","shell.execute_reply.started":"2022-05-08T15:39:32.149230Z","shell.execute_reply":"2022-05-08T15:39:32.157056Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"test = nlp.transform(df_test)","metadata":{"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-05-08T15:39:32.159413Z","iopub.execute_input":"2022-05-08T15:39:32.159708Z","iopub.status.idle":"2022-05-08T15:39:32.841106Z","shell.execute_reply.started":"2022-05-08T15:39:32.159669Z","shell.execute_reply":"2022-05-08T15:39:32.840228Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"loading configuration file ./Name_transformer/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"./Name_transformer\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file ./Name_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./Name_transformer were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at ./Name_transformer and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Transforming column: Name\n","output_type":"stream"}]},{"cell_type":"code","source":"for column in test.columns:\n    df_test[column] = test[column]\ndf_test = df_test.drop(columns=text_columns_name)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:39:32.842574Z","iopub.execute_input":"2022-05-08T15:39:32.842837Z","iopub.status.idle":"2022-05-08T15:39:32.852272Z","shell.execute_reply.started":"2022-05-08T15:39:32.842799Z","shell.execute_reply":"2022-05-08T15:39:32.851318Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df_train.to_csv(f'{emb_mode}_{encode_mode}_autox_trn.csv',index=False)\ndf_test.to_csv(f'{emb_mode}_{encode_mode}_autox_val.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:39:32.854137Z","iopub.execute_input":"2022-05-08T15:39:32.854477Z","iopub.status.idle":"2022-05-08T15:39:32.869496Z","shell.execute_reply.started":"2022-05-08T15:39:32.854413Z","shell.execute_reply":"2022-05-08T15:39:32.868808Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_val=pd.read_csv(f'{emb_mode}_{encode_mode}_autox_val.csv').drop(columns=['Survived'])\ndf_val.to_csv(f'{emb_mode}_{encode_mode}_autox_tst.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:39:38.653379Z","iopub.execute_input":"2022-05-08T15:39:38.653661Z","iopub.status.idle":"2022-05-08T15:39:38.668725Z","shell.execute_reply.started":"2022-05-08T15:39:38.653631Z","shell.execute_reply":"2022-05-08T15:39:38.667403Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from autox import AutoX\n\npath = f'.' \nautox = AutoX(target = 'Survived', train_name = f'{emb_mode}_{encode_mode}_autox_trn.csv', test_name = f'{emb_mode}_{encode_mode}_autox_tst.csv',  id = ['PassengerId'], path = path)\nsub = autox.get_submit()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-08T15:39:41.367691Z","iopub.execute_input":"2022-05-08T15:39:41.367950Z","iopub.status.idle":"2022-05-08T15:39:58.884669Z","shell.execute_reply.started":"2022-05-08T15:39:41.367920Z","shell.execute_reply":"2022-05-08T15:39:58.883947Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"   INFO ->  [+] read Bert_supervise_autox_tst.csv\n   INFO ->  Memory usage of dataframe is 0.03 MB\n   INFO ->  Memory usage after optimization is: 0.02 MB\n   INFO ->  Decreased by 30.9%\n   INFO ->  table = Bert_supervise_autox_tst.csv, shape = (297, 11)\n   INFO ->  [+] read Bert_supervise_autox_trn.csv\n   INFO ->  Memory usage of dataframe is 0.05 MB\n   INFO ->  Memory usage after optimization is: 0.03 MB\n   INFO ->  Decreased by 37.1%\n   INFO ->  table = Bert_supervise_autox_trn.csv, shape = (594, 12)\n   INFO ->  [+] read Bert_supervise_autox_val.csv\n   INFO ->  Memory usage of dataframe is 0.03 MB\n   INFO ->  Memory usage after optimization is: 0.02 MB\n   INFO ->  Decreased by 35.6%\n   INFO ->  table = Bert_supervise_autox_val.csv, shape = (297, 12)\n   INFO ->  [+] read sub_val.csv\n   INFO ->  Memory usage of dataframe is 0.03 MB\n   INFO ->  Memory usage after optimization is: 0.03 MB\n   INFO ->  Decreased by -1.7%\n   INFO ->  table = sub_val.csv, shape = (297, 12)\n   INFO ->  [+] read sub_train.csv\n   INFO ->  Memory usage of dataframe is 0.05 MB\n   INFO ->  Memory usage after optimization is: 0.05 MB\n   INFO ->  Decreased by -0.1%\n   INFO ->  table = sub_train.csv, shape = (594, 12)\n   INFO ->  start feature engineer\n   INFO ->  feature engineer: one2M\n   INFO ->  featureOne2M ops: {}\n   INFO ->  ignore featureOne2M\n   INFO ->  feature engineer: time\n   INFO ->  featureTime ops: []\n0it [00:00, ?it/s]\n   INFO ->  feature engineer: Cumsum\n100%|██████████| 1/1 [00:00<00:00, 88.54it/s]\n   INFO ->  this cols with inf data, del them: ['Cabin__Pclass__cumsum', 'Cabin__Age__cumsum', 'Cabin__SibSp__cumsum', 'Cabin__Parch__cumsum', 'Cabin__Fare__cumsum', 'Cabin__Name_meta_feature__cumsum']\n   INFO ->  featureCumsum ops: {'Cabin': ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Name_meta_feature']}\n   INFO ->  feature engineer: Shift\n100%|██████████| 1/1 [00:00<00:00, 15.61it/s]\n   INFO ->  featureShift ops: {'Cabin': ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Name_meta_feature']}\n   INFO ->  feature engineer: Diff\n100%|██████████| 1/1 [00:00<00:00,  9.25it/s]\n   INFO ->  featureDiff ops: {'Cabin': ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Name_meta_feature']}\n   INFO ->  feature engineer: Stat\n100%|██████████| 1/1 [00:00<00:00, 22.87it/s]\n   INFO ->  featureStat ops: {'Cabin': {'Pclass': ['mean', 'min', 'max', 'median', 'std'], 'Sex': ['nunique'], 'Age': ['mean', 'min', 'max', 'median', 'std'], 'SibSp': ['mean', 'min', 'max', 'median', 'std'], 'Parch': ['mean', 'min', 'max', 'median', 'std'], 'Ticket': ['nunique'], 'Fare': ['mean', 'min', 'max', 'median', 'std'], 'Embarked': ['nunique'], 'Name_meta_feature': ['mean', 'min', 'max', 'median', 'std']}}\n   INFO ->  feature engineer: NLP\n0it [00:00, ?it/s]\n   INFO ->  featureNlp ops: []\n   INFO ->  feature engineer: Count\n100%|██████████| 9/9 [00:00<00:00, 191.36it/s]\n   INFO ->  featureCount ops: [['Sex', 'Cabin'], ['Sex', 'Embarked'], ['Cabin', 'Sex'], ['Cabin', 'Embarked'], ['Embarked', 'Sex'], ['Embarked', 'Cabin'], ['Sex'], ['Cabin'], ['Embarked']]\n   INFO ->  feature engineer: Rank\n100%|██████████| 1/1 [00:00<00:00, 98.54it/s]\n   INFO ->  featureRank ops: {'Cabin': {'PassengerId': ['rank'], 'Survived': ['rank'], 'Pclass': ['rank'], 'Age': ['rank'], 'SibSp': ['rank'], 'Parch': ['rank'], 'Fare': ['rank'], 'Name_meta_feature': ['rank']}}\n   INFO ->  ignore image feature\n100%|██████████| 12/12 [00:00<00:00, 1228.17it/s]\n   INFO ->  label_encoder_list: ['Sex', 'Ticket', 'Cabin', 'Embarked']\n   INFO ->  ordinal_encoder_list: []\n   INFO ->  feature combination\n100%|██████████| 9/9 [00:00<00:00, 773.63it/s]\n   INFO ->  shape of FE_all: (891, 206), shape of train: (594, 206), shape of test: (297, 206)\n   INFO ->  feature filter\n100%|██████████| 202/202 [00:00<00:00, 969.07it/s] \n   INFO ->  filtered features: ['PassengerId', 'Survived', 'Cabin__Pclass__std', 'Cabin__Survived__rank', 'Cabin__Pclass__shift__-30', 'Cabin__Pclass__shift__-24', 'Cabin__Pclass__shift__-7', 'Cabin__Pclass__shift__-3', 'Cabin__Pclass__shift__-2', 'Cabin__Pclass__shift__7', 'Cabin__Pclass__shift__24', 'Cabin__Pclass__shift__30', 'Cabin__Age__shift__-30', 'Cabin__Age__shift__-24', 'Cabin__Age__shift__-7', 'Cabin__Age__shift__-3', 'Cabin__Age__shift__-2', 'Cabin__Age__shift__7', 'Cabin__Age__shift__24', 'Cabin__Age__shift__30', 'Cabin__SibSp__shift__-30', 'Cabin__SibSp__shift__-24', 'Cabin__SibSp__shift__-7', 'Cabin__SibSp__shift__-3', 'Cabin__SibSp__shift__-2', 'Cabin__SibSp__shift__7', 'Cabin__SibSp__shift__24', 'Cabin__SibSp__shift__30', 'Cabin__Parch__shift__-30', 'Cabin__Parch__shift__-24', 'Cabin__Parch__shift__-7', 'Cabin__Parch__shift__-3', 'Cabin__Parch__shift__-2', 'Cabin__Parch__shift__7', 'Cabin__Parch__shift__24', 'Cabin__Parch__shift__30', 'Cabin__Fare__shift__-30', 'Cabin__Fare__shift__-24', 'Cabin__Fare__shift__-7', 'Cabin__Fare__shift__-3', 'Cabin__Fare__shift__-2', 'Cabin__Fare__shift__7', 'Cabin__Fare__shift__24', 'Cabin__Fare__shift__30', 'Cabin__Name_meta_feature__shift__-30', 'Cabin__Name_meta_feature__shift__-24', 'Cabin__Name_meta_feature__shift__-7', 'Cabin__Name_meta_feature__shift__-3', 'Cabin__Name_meta_feature__shift__-2', 'Cabin__Name_meta_feature__shift__7', 'Cabin__Name_meta_feature__shift__24', 'Cabin__Name_meta_feature__shift__30', 'Cabin__Pclass__diff__-30', 'Cabin__Pclass__diff__-24', 'Cabin__Pclass__diff__-7', 'Cabin__Pclass__diff__-3', 'Cabin__Pclass__diff__-2', 'Cabin__Pclass__diff__-1', 'Cabin__Pclass__diff__1', 'Cabin__Pclass__diff__2', 'Cabin__Pclass__diff__7', 'Cabin__Pclass__diff__24', 'Cabin__Pclass__diff__30', 'Cabin__Age__diff__-30', 'Cabin__Age__diff__-24', 'Cabin__Age__diff__-7', 'Cabin__Age__diff__-3', 'Cabin__Age__diff__-2', 'Cabin__Age__diff__7', 'Cabin__Age__diff__24', 'Cabin__Age__diff__30', 'Cabin__SibSp__diff__-30', 'Cabin__SibSp__diff__-24', 'Cabin__SibSp__diff__-7', 'Cabin__SibSp__diff__-3', 'Cabin__SibSp__diff__-2', 'Cabin__SibSp__diff__7', 'Cabin__SibSp__diff__24', 'Cabin__SibSp__diff__30', 'Cabin__Parch__diff__-30', 'Cabin__Parch__diff__-24', 'Cabin__Parch__diff__-7', 'Cabin__Parch__diff__-3', 'Cabin__Parch__diff__-2', 'Cabin__Parch__diff__7', 'Cabin__Parch__diff__24', 'Cabin__Parch__diff__30', 'Cabin__Fare__diff__-30', 'Cabin__Fare__diff__-24', 'Cabin__Fare__diff__-7', 'Cabin__Fare__diff__-3', 'Cabin__Fare__diff__-2', 'Cabin__Fare__diff__7', 'Cabin__Fare__diff__24', 'Cabin__Fare__diff__30', 'Cabin__Name_meta_feature__diff__-30', 'Cabin__Name_meta_feature__diff__-24', 'Cabin__Name_meta_feature__diff__-7', 'Cabin__Name_meta_feature__diff__-3', 'Cabin__Name_meta_feature__diff__-2', 'Cabin__Name_meta_feature__diff__7', 'Cabin__Name_meta_feature__diff__24', 'Cabin__Name_meta_feature__diff__30']\n   INFO ->  used_features: ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Name_meta_feature', 'COUNT_Sex__Cabin', 'COUNT_Sex__Embarked', 'COUNT_Cabin__Sex', 'COUNT_Cabin__Embarked', 'COUNT_Embarked__Sex', 'COUNT_Embarked__Cabin', 'COUNT_Sex', 'COUNT_Cabin', 'COUNT_Embarked', 'Cabin__Pclass__mean', 'Cabin__Pclass__min', 'Cabin__Pclass__max', 'Cabin__Pclass__median', 'Cabin__Sex__nunique', 'Cabin__Age__mean', 'Cabin__Age__min', 'Cabin__Age__max', 'Cabin__Age__median', 'Cabin__Age__std', 'Cabin__SibSp__mean', 'Cabin__SibSp__min', 'Cabin__SibSp__max', 'Cabin__SibSp__median', 'Cabin__SibSp__std', 'Cabin__Parch__mean', 'Cabin__Parch__min', 'Cabin__Parch__max', 'Cabin__Parch__median', 'Cabin__Parch__std', 'Cabin__Ticket__nunique', 'Cabin__Fare__mean', 'Cabin__Fare__min', 'Cabin__Fare__max', 'Cabin__Fare__median', 'Cabin__Fare__std', 'Cabin__Embarked__nunique', 'Cabin__Name_meta_feature__mean', 'Cabin__Name_meta_feature__min', 'Cabin__Name_meta_feature__max', 'Cabin__Name_meta_feature__median', 'Cabin__Name_meta_feature__std', 'Cabin__PassengerId__rank', 'Cabin__Pclass__rank', 'Cabin__Age__rank', 'Cabin__SibSp__rank', 'Cabin__Parch__rank', 'Cabin__Fare__rank', 'Cabin__Name_meta_feature__rank', 'Cabin__Pclass__shift__-1', 'Cabin__Pclass__shift__1', 'Cabin__Pclass__shift__2', 'Cabin__Pclass__shift__3', 'Cabin__Age__shift__-1', 'Cabin__Age__shift__1', 'Cabin__Age__shift__2', 'Cabin__Age__shift__3', 'Cabin__SibSp__shift__-1', 'Cabin__SibSp__shift__1', 'Cabin__SibSp__shift__2', 'Cabin__SibSp__shift__3', 'Cabin__Parch__shift__-1', 'Cabin__Parch__shift__1', 'Cabin__Parch__shift__2', 'Cabin__Parch__shift__3', 'Cabin__Fare__shift__-1', 'Cabin__Fare__shift__1', 'Cabin__Fare__shift__2', 'Cabin__Fare__shift__3', 'Cabin__Name_meta_feature__shift__-1', 'Cabin__Name_meta_feature__shift__1', 'Cabin__Name_meta_feature__shift__2', 'Cabin__Name_meta_feature__shift__3', 'Cabin__Pclass__diff__3', 'Cabin__Age__diff__-1', 'Cabin__Age__diff__1', 'Cabin__Age__diff__2', 'Cabin__Age__diff__3', 'Cabin__SibSp__diff__-1', 'Cabin__SibSp__diff__1', 'Cabin__SibSp__diff__2', 'Cabin__SibSp__diff__3', 'Cabin__Parch__diff__-1', 'Cabin__Parch__diff__1', 'Cabin__Parch__diff__2', 'Cabin__Parch__diff__3', 'Cabin__Fare__diff__-1', 'Cabin__Fare__diff__1', 'Cabin__Fare__diff__2', 'Cabin__Fare__diff__3', 'Cabin__Name_meta_feature__diff__-1', 'Cabin__Name_meta_feature__diff__1', 'Cabin__Name_meta_feature__diff__2', 'Cabin__Name_meta_feature__diff__3']\n   INFO ->  start training lightgbm model\n   INFO ->  (594, 99)\n","output_type":"stream"},{"name":"stdout","text":"Training on fold 1\nTraining until validation scores don't improve for 150 rounds\n[100]\ttraining's auc: 0.929411\tvalid_1's auc: 0.863327\n[200]\ttraining's auc: 0.940415\tvalid_1's auc: 0.866729\n[300]\ttraining's auc: 0.953514\tvalid_1's auc: 0.865492\nEarly stopping, best iteration is:\n[190]\ttraining's auc: 0.939254\tvalid_1's auc: 0.867965\nAUC: 0.867965367965368\nFold 1 finished in 0:00:00.184559\nTraining on fold 2\nTraining until validation scores don't improve for 150 rounds\n[100]\ttraining's auc: 0.93161\tvalid_1's auc: 0.873574\n[200]\ttraining's auc: 0.944124\tvalid_1's auc: 0.866967\nEarly stopping, best iteration is:\n[51]\ttraining's auc: 0.9242\tvalid_1's auc: 0.877928\nAUC: 0.877927927927928\nFold 2 finished in 0:00:00.103879\nTraining on fold 3\nTraining until validation scores don't improve for 150 rounds\n[100]\ttraining's auc: 0.928303\tvalid_1's auc: 0.909615\nEarly stopping, best iteration is:\n[4]\ttraining's auc: 0.913476\tvalid_1's auc: 0.926603\nAUC: 0.9266025641025641\nFold 3 finished in 0:00:00.081515\nTraining on fold 4\nTraining until validation scores don't improve for 150 rounds\n[100]\ttraining's auc: 0.935663\tvalid_1's auc: 0.866097\n","output_type":"stream"},{"name":"stderr","text":"   INFO ->  feature importance\n   INFO ->                                feature  fold_1  fold_2  fold_3  fold_4  fold_5  \\\n0                                 Age     323      80       9     411       6   \n1                                Fare     307      84      11     286       9   \n2                   Name_meta_feature     208      76      10     368       7   \n3            Cabin__PassengerId__rank     190      51       6     332       2   \n4                    Cabin__Age__rank     164      59       2     263       3   \n..                                ...     ...     ...     ...     ...     ...   \n94             Cabin__Parch__shift__2       0       0       0       0       0   \n95             Cabin__Parch__shift__3       0       0       0       0       0   \n96             Cabin__Fare__shift__-1       0       0       0       0       0   \n97              Cabin__Fare__shift__1       0       0       0       0       0   \n98  Cabin__Name_meta_feature__diff__3       0       0       0       0       0   \n\n    average  \n0     165.8  \n1     139.4  \n2     133.8  \n3     116.2  \n4      98.2  \n..      ...  \n94      0.0  \n95      0.0  \n96      0.0  \n97      0.0  \n98      0.0  \n\n[99 rows x 7 columns]\n   INFO ->  start training xgboost model\n   INFO ->  (594, 99)\n","output_type":"stream"},{"name":"stdout","text":"[200]\ttraining's auc: 0.946426\tvalid_1's auc: 0.870085\n[300]\ttraining's auc: 0.958449\tvalid_1's auc: 0.87094\nEarly stopping, best iteration is:\n[227]\ttraining's auc: 0.95035\tvalid_1's auc: 0.873219\nAUC: 0.8732193732193733\nFold 4 finished in 0:00:00.173914\nTraining on fold 5\nTraining until validation scores don't improve for 150 rounds\n[100]\ttraining's auc: 0.942005\tvalid_1's auc: 0.796347\nEarly stopping, best iteration is:\n[3]\ttraining's auc: 0.926839\tvalid_1's auc: 0.812481\nAUC: 0.8124809741248098\nFold 5 finished in 0:00:00.098182\nTraining on fold 1\nAUC: 0.8478260869565217\nFold 1 finished in 0:00:02.750917\nTraining on fold 2\nAUC: 0.8061104582843713\nFold 2 finished in 0:00:01.379876\nTraining on fold 3\nAUC: 0.8789682539682541\nFold 3 finished in 0:00:01.457884\nTraining on fold 4\nAUC: 0.7579318448883665\nFold 4 finished in 0:00:01.409821\nTraining on fold 5\nAUC: 0.8518518518518519\nFold 5 finished in 0:00:01.391494\nTraining on fold 6\nAUC: 0.8082706766917294\nFold 6 finished in 0:00:01.356802\nTraining on fold 7\nAUC: 0.8035714285714286\nFold 7 finished in 0:00:01.411690\nTraining on fold 8\nAUC: 0.7706766917293233\nFold 8 finished in 0:00:01.455820\nTraining on fold 9\nAUC: 0.8288461538461539\nFold 9 finished in 0:00:01.592738\nTraining on fold 10\n","output_type":"stream"},{"name":"stderr","text":"   INFO ->  Average KFold AUC: 0.8142876976199765\n","output_type":"stream"},{"name":"stdout","text":"AUC: 0.7888235294117647\nFold 10 finished in 0:00:01.387209\n","output_type":"stream"}]},{"cell_type":"code","source":"val = pd.read_csv(f'sub_val.csv')\nfrom sklearn.metrics import mean_squared_error,roc_auc_score\nAUC = (roc_auc_score(val['Survived'], sub['Survived']))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:39:58.886196Z","iopub.execute_input":"2022-05-08T15:39:58.886869Z","iopub.status.idle":"2022-05-08T15:39:58.898257Z","shell.execute_reply.started":"2022-05-08T15:39:58.886827Z","shell.execute_reply":"2022-05-08T15:39:58.897433Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"AUC","metadata":{"execution":{"iopub.status.busy":"2022-05-08T15:40:03.646562Z","iopub.execute_input":"2022-05-08T15:40:03.647026Z","iopub.status.idle":"2022-05-08T15:40:03.652453Z","shell.execute_reply.started":"2022-05-08T15:40:03.646982Z","shell.execute_reply":"2022-05-08T15:40:03.651761Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"0.8578347578347577"},"metadata":{}}]}]}