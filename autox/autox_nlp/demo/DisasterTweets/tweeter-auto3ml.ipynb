{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import KFold\n\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:01:44.857228Z","iopub.execute_input":"2022-05-08T17:01:44.857593Z","iopub.status.idle":"2022-05-08T17:01:45.889885Z","shell.execute_reply.started":"2022-05-08T17:01:44.857503Z","shell.execute_reply":"2022-05-08T17:01:45.888883Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=3, random_state=1001,shuffle=True)\nfor i, (train_index, val_index) in enumerate(kf.split(train)):\n    trn= train.iloc[train_index].reset_index()\n    val= train.iloc[val_index].reset_index()\n    \ntrn = trn.drop(columns=['index'])\nval = val.drop(columns=['index'])\n\nval.to_csv('sub_val.csv',index=False)\ntrn.to_csv('sub_train.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:01:45.897560Z","iopub.execute_input":"2022-05-08T17:01:45.897847Z","iopub.status.idle":"2022-05-08T17:01:46.001100Z","shell.execute_reply.started":"2022-05-08T17:01:45.897816Z","shell.execute_reply":"2022-05-08T17:01:46.000055Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# !rm -r ./autox\n!git clone https://github.com/4paradigm/autox.git\n!pip install ./autox","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-08T17:01:46.005752Z","iopub.execute_input":"2022-05-08T17:01:46.006008Z","iopub.status.idle":"2022-05-08T17:02:25.264982Z","shell.execute_reply.started":"2022-05-08T17:01:46.005976Z","shell.execute_reply":"2022-05-08T17:02:25.264147Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'autox'...\nremote: Enumerating objects: 2704, done.\u001b[K\nremote: Counting objects: 100% (617/617), done.\u001b[K\nremote: Compressing objects: 100% (153/153), done.\u001b[K\nremote: Total 2704 (delta 483), reused 576 (delta 457), pack-reused 2087\u001b[K\nReceiving objects: 100% (2704/2704), 11.87 MiB | 17.91 MiB/s, done.\nResolving deltas: 100% (1727/1727), done.\nProcessing ./autox\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: lightgbm in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (3.3.1)\nRequirement already satisfied: xgboost in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (1.6.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (1.9.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (1.21.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (1.3.5)\nRequirement already satisfied: sklearn in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (4.63.0)\nRequirement already satisfied: optuna in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (2.10.0)\nCollecting img2vec_pytorch\n  Downloading img2vec_pytorch-1.0.1-py3-none-any.whl (6.9 kB)\nCollecting pypinyin\n  Downloading pypinyin-0.46.0-py2.py3-none-any.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hRequirement already satisfied: keras in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (2.6.0)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (2.6.3)\nRequirement already satisfied: gensim in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (4.0.1)\nCollecting glove-python-binary\n  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.9/948.9 KB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from automl-x==0.3.0) (4.18.0)\nCollecting datasets\n  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.4/325.4 KB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (0.3.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (2.27.1)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (7.0.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (0.5.1)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (2022.3.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (4.11.3)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (0.70.12.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (21.3)\nCollecting responses<0.19\n  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets->automl-x==0.3.0) (3.8.1)\nCollecting xxhash\n  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim->automl-x==0.3.0) (1.7.3)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim->automl-x==0.3.0) (5.2.1)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from img2vec_pytorch->automl-x==0.3.0) (0.10.1)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from lightgbm->automl-x==0.3.0) (0.37.1)\nRequirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.7/site-packages (from lightgbm->automl-x==0.3.0) (1.0.2)\nRequirement already satisfied: cmaes>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (0.8.2)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (6.0)\nRequirement already satisfied: alembic in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (1.7.7)\nRequirement already satisfied: cliff in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (3.10.1)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (6.6.0)\nRequirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from optuna->automl-x==0.3.0) (1.4.32)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->automl-x==0.3.0) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->automl-x==0.3.0) (2021.3)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (0.4.0)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (5.0)\nCollecting six~=1.15.0\n  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (3.3.0)\nCollecting numpy\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting absl-py~=0.10\n  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (1.1.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (1.1.2)\nCollecting wrapt~=1.12.1\n  Downloading wrapt-1.12.1.tar.gz (27 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (3.1.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (0.2.0)\nRequirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (2.6.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (1.12)\nRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (2.6.0)\nCollecting typing-extensions<3.11,>=3.7\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (1.43.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (1.6.3)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->automl-x==0.3.0) (3.19.4)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers->automl-x==0.3.0) (0.0.49)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers->automl-x==0.3.0) (0.12.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers->automl-x==0.3.0) (3.6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers->automl-x==0.3.0) (2021.11.10)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow->automl-x==0.3.0) (1.5.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets->automl-x==0.3.0) (3.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->automl-x==0.3.0) (2021.10.8)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->automl-x==0.3.0) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->automl-x==0.3.0) (1.26.8)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->automl-x==0.3.0) (2.0.12)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm->automl-x==0.3.0) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm->automl-x==0.3.0) (1.0.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna->automl-x==0.3.0) (1.1.2)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (59.8.0)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (0.6.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (1.8.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (3.3.6)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (0.4.6)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (2.0.3)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (1.35.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (1.7.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (21.4.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (4.0.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (0.13.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (1.3.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->automl-x==0.3.0) (1.2.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from alembic->optuna->automl-x==0.3.0) (5.4.0)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic->optuna->automl-x==0.3.0) (1.2.0)\nRequirement already satisfied: cmd2>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna->automl-x==0.3.0) (2.4.1)\nRequirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna->automl-x==0.3.0) (3.2.0)\nRequirement already satisfied: stevedore>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna->automl-x==0.3.0) (3.5.0)\nRequirement already satisfied: autopage>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna->automl-x==0.3.0) (0.5.0)\nRequirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna->automl-x==0.3.0) (5.8.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets->automl-x==0.3.0) (3.7.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers->automl-x==0.3.0) (8.0.4)\nRequirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->img2vec_pytorch->automl-x==0.3.0) (9.0.1)\nRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna->automl-x==0.3.0) (0.2.5)\nRequirement already satisfied: pyperclip>=1.6 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna->automl-x==0.3.0) (1.8.2)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (4.8)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (1.3.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic->optuna->automl-x==0.3.0) (2.0.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow->automl-x==0.3.0) (3.2.0)\nBuilding wheels for collected packages: automl-x, wrapt\n  Building wheel for automl-x (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for automl-x: filename=automl_x-0.3.0-py3-none-any.whl size=4809780 sha256=278a8217a26bbaccae133b356fd9114329114f463d7e149aad0903cbb6c49a04\n  Stored in directory: /tmp/pip-ephem-wheel-cache-nbw5nxt9/wheels/d8/d0/3f/17c06e69ec61f9c250a3274a013cc8a24b47965ec33f40c73d\n  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=77054 sha256=2845d6ebf96e10ae32cc597d8b174aba4816bb7a8a9ad0adc9bb3f4b4eb4ea1b\n  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\nSuccessfully built automl-x wrapt\nInstalling collected packages: wrapt, typing-extensions, xxhash, six, pypinyin, numpy, responses, absl-py, img2vec_pytorch, glove-python-binary, datasets, automl-x\n  Attempting uninstall: wrapt\n    Found existing installation: wrapt 1.14.0\n    Uninstalling wrapt-1.14.0:\n      Successfully uninstalled wrapt-1.14.0\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.2.0\n    Uninstalling typing_extensions-4.2.0:\n      Successfully uninstalled typing_extensions-4.2.0\n  Attempting uninstall: six\n    Found existing installation: six 1.16.0\n    Uninstalling six-1.16.0:\n      Successfully uninstalled six-1.16.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: absl-py\n    Found existing installation: absl-py 1.0.0\n    Uninstalling absl-py-1.0.0:\n      Successfully uninstalled absl-py-1.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\ntfx-bsl 1.7.0 requires pyarrow<6,>=1, but you have pyarrow 7.0.0 which is incompatible.\ntfx-bsl 1.7.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5, but you have tensorflow 2.6.3 which is incompatible.\ntensorflow-transform 1.7.0 requires pyarrow<6,>=1, but you have pyarrow 7.0.0 which is incompatible.\ntensorflow-transform 1.7.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<2.9,>=1.15.5, but you have tensorflow 2.6.3 which is incompatible.\ntensorflow-serving-api 2.8.0 requires tensorflow<3,>=2.8.0, but you have tensorflow 2.6.3 which is incompatible.\nrich 12.2.0 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytorch-lightning 1.6.1 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npytools 2022.1.5 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.1 which is incompatible.\nimageio 2.16.1 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\ngrpcio-status 1.44.0 requires grpcio>=1.44.0, but you have grpcio 1.43.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-cloud-storage<2.0.0dev,>=1.26.0, but you have google-cloud-storage 2.1.0 which is incompatible.\ngcsfs 2022.2.0 requires fsspec==2022.02.0, but you have fsspec 2022.3.0 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.11.3 which is incompatible.\nfeaturetools 1.8.0 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\napache-beam 2.37.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\napache-beam 2.37.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.4 which is incompatible.\napache-beam 2.37.0 requires pyarrow<7.0.0,>=0.15.1, but you have pyarrow 7.0.0 which is incompatible.\naioitertools 0.10.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.2.0 requires botocore<1.24.22,>=1.24.21, but you have botocore 1.25.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed absl-py-0.15.0 automl-x-0.3.0 datasets-2.1.0 glove-python-binary-0.2.0 img2vec_pytorch-1.0.1 numpy-1.19.5 pypinyin-0.46.0 responses-0.18.0 six-1.15.0 typing-extensions-3.10.0.2 wrapt-1.12.1 xxhash-3.0.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from autox.autox_nlp import NLP_feature\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:02:25.267505Z","iopub.execute_input":"2022-05-08T17:02:25.267791Z","iopub.status.idle":"2022-05-08T17:02:35.703071Z","shell.execute_reply.started":"2022-05-08T17:02:25.267755Z","shell.execute_reply":"2022-05-08T17:02:35.702255Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"df_train = pd.read_csv('sub_train.csv')\ndf_test = pd.read_csv('sub_val.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:02:35.704358Z","iopub.execute_input":"2022-05-08T17:02:35.704640Z","iopub.status.idle":"2022-05-08T17:02:35.737132Z","shell.execute_reply.started":"2022-05-08T17:02:35.704605Z","shell.execute_reply":"2022-05-08T17:02:35.736459Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"use_Toknizer=True\nemb_mode = 'Bert'# TFIDF / Word2Vec / Glove / FastText / Bert\nencode_mode = 'supervise' # unsupervise / supervise\ntext_columns_name = ['text']\ntarget_column = df_train['target']\ncandidate_labels=None","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:02:35.738468Z","iopub.execute_input":"2022-05-08T17:02:35.738738Z","iopub.status.idle":"2022-05-08T17:02:35.744888Z","shell.execute_reply.started":"2022-05-08T17:02:35.738703Z","shell.execute_reply":"2022-05-08T17:02:35.744077Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"nlp = NLP_feature()\n# nlp.do_mlm = True\n# nlp.mlm_epochs=3\n# nlp.model_name = 'microsoft/deberta-v3-base'\nnlp.emb_size=100\nnlp.n_clusters=20\ndf = nlp.fit(df_train,\n             text_columns_name,\n             use_Toknizer,\n             emb_mode,\n             encode_mode,\n             target_column,\n             candidate_labels)","metadata":{"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-05-08T17:02:35.746338Z","iopub.execute_input":"2022-05-08T17:02:35.746667Z","iopub.status.idle":"2022-05-08T17:03:45.734889Z","shell.execute_reply.started":"2022-05-08T17:02:35.746629Z","shell.execute_reply":"2022-05-08T17:03:45.734129Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Fitting column: text tokenizer\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/285 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3686d84a75f4d84a2d1d6446dc8d656"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"720d35ea691b47aab64e60fea3f8c2cf"}},"metadata":{}},{"name":"stdout","text":"Fitting column: text bert embedding\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/16.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d2351f9b3b54c97a1497758a08f00b8"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSaving model checkpoint to ./text_transformer\nConfiguration saved in ./text_transformer/config.json\nModel weights saved in ./text_transformer/pytorch_model.bin\nloading configuration file ./text_transformer/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"./text_transformer\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file ./text_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./text_transformer were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at ./text_transformer and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Fitting column: text encoder\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file ./text_transformer/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"./text_transformer\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file ./text_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./text_transformer were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at ./text_transformer and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nloading configuration file ./text_transformer/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"./text_transformer\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file ./text_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./text_transformer were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at ./text_transformer and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nloading configuration file ./text_transformer/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"./text_transformer\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file ./text_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./text_transformer were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at ./text_transformer and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nloading configuration file ./text_transformer/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"./text_transformer\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file ./text_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./text_transformer were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at ./text_transformer and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"for column in df.columns:\n    df_train[column] = df[column]\ndf_train = df_train.drop(columns=text_columns_name)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:03:45.736061Z","iopub.execute_input":"2022-05-08T17:03:45.736309Z","iopub.status.idle":"2022-05-08T17:03:45.744632Z","shell.execute_reply.started":"2022-05-08T17:03:45.736276Z","shell.execute_reply":"2022-05-08T17:03:45.743290Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"test = nlp.transform(df_test)","metadata":{"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-05-08T17:03:45.745818Z","iopub.execute_input":"2022-05-08T17:03:45.746278Z","iopub.status.idle":"2022-05-08T17:03:51.227885Z","shell.execute_reply.started":"2022-05-08T17:03:45.746241Z","shell.execute_reply":"2022-05-08T17:03:51.227173Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"loading configuration file ./text_transformer/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"./text_transformer\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"output_hidden_states\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file ./text_transformer/pytorch_model.bin\nSome weights of the model checkpoint at ./text_transformer were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at ./text_transformer and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Transforming column: text\n","output_type":"stream"}]},{"cell_type":"code","source":"for column in test.columns:\n    df_test[column] = test[column]\ndf_test = df_test.drop(columns=text_columns_name)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:03:51.230576Z","iopub.execute_input":"2022-05-08T17:03:51.230819Z","iopub.status.idle":"2022-05-08T17:03:51.238735Z","shell.execute_reply.started":"2022-05-08T17:03:51.230786Z","shell.execute_reply":"2022-05-08T17:03:51.236533Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df_train.to_csv(f'{emb_mode}_{encode_mode}_autox_trn.csv',index=False)\ndf_test.to_csv(f'{emb_mode}_{encode_mode}_autox_val.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:03:51.240202Z","iopub.execute_input":"2022-05-08T17:03:51.240587Z","iopub.status.idle":"2022-05-08T17:03:51.280834Z","shell.execute_reply.started":"2022-05-08T17:03:51.240449Z","shell.execute_reply":"2022-05-08T17:03:51.280135Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_val=pd.read_csv(f'{emb_mode}_{encode_mode}_autox_val.csv').drop(columns=['target'])\ndf_val.to_csv(f'{emb_mode}_{encode_mode}_autox_tst.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:03:51.282098Z","iopub.execute_input":"2022-05-08T17:03:51.282394Z","iopub.status.idle":"2022-05-08T17:03:51.303336Z","shell.execute_reply.started":"2022-05-08T17:03:51.282357Z","shell.execute_reply":"2022-05-08T17:03:51.302718Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from autox import AutoX\n\npath = f'.' \nautox = AutoX(target = 'target', train_name = f'{emb_mode}_{encode_mode}_autox_trn.csv', test_name = f'{emb_mode}_{encode_mode}_autox_tst.csv',  id = ['id'], path = path)\nsub = autox.get_submit()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-08T17:03:51.304301Z","iopub.execute_input":"2022-05-08T17:03:51.306375Z","iopub.status.idle":"2022-05-08T17:08:05.884103Z","shell.execute_reply.started":"2022-05-08T17:03:51.306345Z","shell.execute_reply":"2022-05-08T17:08:05.883239Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"   INFO ->  [+] read sub_val.csv\n   INFO ->  Memory usage of dataframe is 0.10 MB\n   INFO ->  Memory usage after optimization is: 0.15 MB\n   INFO ->  Decreased by -59.8%\n   INFO ->  table = sub_val.csv, shape = (2537, 5)\n   INFO ->  [+] read Bert_supervise_autox_tst.csv\n   INFO ->  Memory usage of dataframe is 0.08 MB\n   INFO ->  Memory usage after optimization is: 0.07 MB\n   INFO ->  Decreased by 9.5%\n   INFO ->  table = Bert_supervise_autox_tst.csv, shape = (2537, 4)\n   INFO ->  [+] read sub_train.csv\n   INFO ->  Memory usage of dataframe is 0.19 MB\n   INFO ->  Memory usage after optimization is: 0.30 MB\n   INFO ->  Decreased by -54.3%\n   INFO ->  table = sub_train.csv, shape = (5076, 5)\n   INFO ->  [+] read Bert_supervise_autox_trn.csv\n   INFO ->  Memory usage of dataframe is 0.19 MB\n   INFO ->  Memory usage after optimization is: 0.13 MB\n   INFO ->  Decreased by 30.5%\n   INFO ->  table = Bert_supervise_autox_trn.csv, shape = (5076, 5)\n   INFO ->  [+] read Bert_supervise_autox_val.csv\n   INFO ->  Memory usage of dataframe is 0.10 MB\n   INFO ->  Memory usage after optimization is: 0.07 MB\n   INFO ->  Decreased by 25.1%\n   INFO ->  table = Bert_supervise_autox_val.csv, shape = (2537, 5)\n   INFO ->  start feature engineer\n   INFO ->  feature engineer: one2M\n   INFO ->  featureOne2M ops: {}\n   INFO ->  ignore featureOne2M\n   INFO ->  feature engineer: time\n   INFO ->  featureTime ops: []\n0it [00:00, ?it/s]\n   INFO ->  feature engineer: Cumsum\n100%|██████████| 1/1 [00:00<00:00, 187.86it/s]\n   INFO ->  this cols with inf data, del them: ['keyword__text_meta_feature__cumsum']\n   INFO ->  featureCumsum ops: {'keyword': ['text_meta_feature']}\n   INFO ->  feature engineer: Shift\n100%|██████████| 1/1 [00:00<00:00, 78.65it/s]\n   INFO ->  featureShift ops: {'keyword': ['text_meta_feature']}\n   INFO ->  feature engineer: Diff\n100%|██████████| 1/1 [00:00<00:00, 54.23it/s]\n   INFO ->  featureDiff ops: {'keyword': ['text_meta_feature']}\n   INFO ->  feature engineer: Stat\n100%|██████████| 1/1 [00:00<00:00, 59.64it/s]\n   INFO ->  featureStat ops: {'keyword': {'location': ['nunique'], 'text_meta_feature': ['mean', 'min', 'max', 'median', 'std']}}\n   INFO ->  feature engineer: NLP\n0it [00:00, ?it/s]\n   INFO ->  featureNlp ops: []\n   INFO ->  feature engineer: Count\n100%|██████████| 1/1 [00:00<00:00, 150.92it/s]\n   INFO ->  featureCount ops: [['keyword']]\n   INFO ->  feature engineer: Rank\n100%|██████████| 1/1 [00:00<00:00, 127.77it/s]\n   INFO ->  featureRank ops: {'keyword': {'id': ['rank'], 'target': ['rank'], 'text_meta_feature': ['rank']}}\n   INFO ->  ignore image feature\n100%|██████████| 5/5 [00:00<00:00, 426.24it/s]\n   INFO ->  label_encoder_list: ['keyword', 'location']\n   INFO ->  ordinal_encoder_list: []\n   INFO ->  feature combination\n100%|██████████| 9/9 [00:00<00:00, 1798.16it/s]\n   INFO ->  shape of FE_all: (7613, 39), shape of train: (5076, 39), shape of test: (2537, 39)\n   INFO ->  feature filter\n100%|██████████| 37/37 [00:00<00:00, 727.72it/s]\n   INFO ->  filtered features: ['id', 'target', 'keyword__target__rank', 'keyword__text_meta_feature__shift__-30', 'keyword__text_meta_feature__shift__-24', 'keyword__text_meta_feature__diff__-30', 'keyword__text_meta_feature__diff__-24']\n   INFO ->  used_features: ['text_meta_feature', 'COUNT_keyword', 'keyword__location__nunique', 'keyword__text_meta_feature__mean', 'keyword__text_meta_feature__min', 'keyword__text_meta_feature__max', 'keyword__text_meta_feature__median', 'keyword__text_meta_feature__std', 'keyword__id__rank', 'keyword__text_meta_feature__rank', 'keyword__text_meta_feature__shift__-7', 'keyword__text_meta_feature__shift__-3', 'keyword__text_meta_feature__shift__-2', 'keyword__text_meta_feature__shift__-1', 'keyword__text_meta_feature__shift__1', 'keyword__text_meta_feature__shift__2', 'keyword__text_meta_feature__shift__3', 'keyword__text_meta_feature__shift__7', 'keyword__text_meta_feature__shift__24', 'keyword__text_meta_feature__shift__30', 'keyword__text_meta_feature__diff__-7', 'keyword__text_meta_feature__diff__-3', 'keyword__text_meta_feature__diff__-2', 'keyword__text_meta_feature__diff__-1', 'keyword__text_meta_feature__diff__1', 'keyword__text_meta_feature__diff__2', 'keyword__text_meta_feature__diff__3', 'keyword__text_meta_feature__diff__7', 'keyword__text_meta_feature__diff__24', 'keyword__text_meta_feature__diff__30']\n   INFO ->  start training lightgbm model\n   INFO ->  (5076, 30)\n","output_type":"stream"},{"name":"stdout","text":"Training on fold 1\nTraining until validation scores don't improve for 150 rounds\n[100]\ttraining's auc: 0.892195\tvalid_1's auc: 0.844218\n[200]\ttraining's auc: 0.90213\tvalid_1's auc: 0.844091\nEarly stopping, best iteration is:\n[68]\ttraining's auc: 0.889497\tvalid_1's auc: 0.845478\nAUC: 0.8454781836631151\nFold 1 finished in 0:00:00.455650\nTraining on fold 2\nTraining until validation scores don't improve for 150 rounds\n[100]\ttraining's auc: 0.888237\tvalid_1's auc: 0.863709\n[200]\ttraining's auc: 0.898949\tvalid_1's auc: 0.864454\n[300]\ttraining's auc: 0.909428\tvalid_1's auc: 0.863854\nEarly stopping, best iteration is:\n[199]\ttraining's auc: 0.898871\tvalid_1's auc: 0.864553\nAUC: 0.864552590266876\nFold 2 finished in 0:00:00.663597\nTraining on fold 3\nTraining until validation scores don't improve for 150 rounds\n[100]\ttraining's auc: 0.89626\tvalid_1's auc: 0.827471\n[200]\ttraining's auc: 0.906524\tvalid_1's auc: 0.828983\n[300]\ttraining's auc: 0.917246\tvalid_1's auc: 0.828651\n[400]\ttraining's auc: 0.925924\tvalid_1's auc: 0.829594\n[500]\ttraining's auc: 0.934708\tvalid_1's auc: 0.829918\n[600]\ttraining's auc: 0.943203\tvalid_1's auc: 0.829474\nEarly stopping, best iteration is:\n[470]\ttraining's auc: 0.932046\tvalid_1's auc: 0.82999\nAUC: 0.8299903228644322\nFold 3 finished in 0:00:01.107103\nTraining on fold 4\nTraining until validation scores don't improve for 150 rounds\n[100]\ttraining's auc: 0.890588\tvalid_1's auc: 0.854255\n[200]\ttraining's auc: 0.901038\tvalid_1's auc: 0.854992\n[300]\ttraining's auc: 0.911889\tvalid_1's auc: 0.855239\n[400]\ttraining's auc: 0.920158\tvalid_1's auc: 0.854677\nEarly stopping, best iteration is:\n[264]\ttraining's auc: 0.908076\tvalid_1's auc: 0.855685\nAUC: 0.8556851311953352\nFold 4 finished in 0:00:00.755277\nTraining on fold 5\nTraining until validation scores don't improve for 150 rounds\n[100]\ttraining's auc: 0.890411\tvalid_1's auc: 0.852703\n[200]\ttraining's auc: 0.900801\tvalid_1's auc: 0.854385\n","output_type":"stream"},{"name":"stderr","text":"   INFO ->  feature importance\n   INFO ->                                    feature  fold_1  fold_2  fold_3  fold_4  \\\n0                       text_meta_feature     276     693     990     866   \n1        keyword__text_meta_feature__mean     179     495     768     537   \n2      keyword__text_meta_feature__median     125     409     543     465   \n3         keyword__text_meta_feature__std     118     281     529     283   \n4         keyword__text_meta_feature__min      81     273     562     358   \n5         keyword__text_meta_feature__max     107     233     518     283   \n6    keyword__text_meta_feature__shift__1      41     195     457     311   \n7     keyword__text_meta_feature__diff__2      55     156     402     240   \n8     keyword__text_meta_feature__diff__1      45     166     403     241   \n9    keyword__text_meta_feature__diff__-3      78     157     395     230   \n10    keyword__text_meta_feature__diff__3      58     191     307     237   \n11  keyword__text_meta_feature__shift__-3      55     194     338     244   \n12   keyword__text_meta_feature__diff__-1      54     146     377     218   \n13  keyword__text_meta_feature__shift__-7      40     150     378     264   \n14   keyword__text_meta_feature__shift__2      48     159     431     192   \n15   keyword__text_meta_feature__diff__-7      41     177     290     253   \n16       keyword__text_meta_feature__rank      75     156     245     211   \n17  keyword__text_meta_feature__shift__-2      55     129     310     246   \n18  keyword__text_meta_feature__shift__-1      36     125     337     172   \n19   keyword__text_meta_feature__diff__-2      62     143     310     160   \n20    keyword__text_meta_feature__diff__7      57     103     329     167   \n21   keyword__text_meta_feature__shift__3      69     157     295     168   \n22                          COUNT_keyword      51     102     379     145   \n23   keyword__text_meta_feature__shift__7      48     124     274     156   \n24             keyword__location__nunique      43      88     256     153   \n25                      keyword__id__rank      20      69     219      84   \n26  keyword__text_meta_feature__shift__24      11      15     106      48   \n27   keyword__text_meta_feature__diff__24       1      11      14      28   \n28  keyword__text_meta_feature__shift__30       0       0       0       0   \n29   keyword__text_meta_feature__diff__30       0       0       0       0   \n\n    fold_5  average  \n0      709    706.8  \n1      498    495.4  \n2      378    384.0  \n3      309    304.0  \n4      207    296.2  \n5      275    283.2  \n6      199    240.6  \n7      285    227.6  \n8      208    212.6  \n9      187    209.4  \n10     238    206.2  \n11     184    203.0  \n12     217    202.4  \n13     155    197.4  \n14     144    194.8  \n15     156    183.4  \n16     187    174.8  \n17     133    174.6  \n18     187    171.4  \n19     173    169.6  \n20     189    169.0  \n21     155    168.8  \n22     147    164.8  \n23     129    146.2  \n24      81    124.2  \n25      60     90.4  \n26      32     42.4  \n27       7     12.2  \n28       0      0.0  \n29       0      0.0  \n   INFO ->  start training xgboost model\n   INFO ->  (5076, 30)\n","output_type":"stream"},{"name":"stdout","text":"[300]\ttraining's auc: 0.910134\tvalid_1's auc: 0.854819\nEarly stopping, best iteration is:\n[219]\ttraining's auc: 0.903073\tvalid_1's auc: 0.855008\nAUC: 0.8550075621514321\nFold 5 finished in 0:00:00.752540\nTraining on fold 1\nAUC: 0.832623106060606\nFold 1 finished in 0:00:26.269679\nTraining on fold 2\nAUC: 0.8695001207437817\nFold 2 finished in 0:00:24.650367\nTraining on fold 3\nAUC: 0.8519242593848044\nFold 3 finished in 0:00:24.435282\nTraining on fold 4\nAUC: 0.8517622614008157\nFold 4 finished in 0:00:24.482831\nTraining on fold 5\nAUC: 0.8591003591003591\nFold 5 finished in 0:00:24.430619\nTraining on fold 6\nAUC: 0.8478044871794872\nFold 6 finished in 0:00:24.837828\nTraining on fold 7\nAUC: 0.8613715552739944\nFold 7 finished in 0:00:24.808846\nTraining on fold 8\nAUC: 0.8350846004888733\nFold 8 finished in 0:00:24.513068\nTraining on fold 9\nAUC: 0.8337787959781087\nFold 9 finished in 0:00:25.618585\nTraining on fold 10\n","output_type":"stream"},{"name":"stderr","text":"   INFO ->  Average KFold AUC: 0.8449988695208963\n","output_type":"stream"},{"name":"stdout","text":"AUC: 0.8070391495981333\nFold 10 finished in 0:00:25.343297\n","output_type":"stream"}]},{"cell_type":"code","source":"val = pd.read_csv(f'sub_val.csv')\nfrom sklearn.metrics import mean_squared_error,roc_auc_score\nAUC = (roc_auc_score(val['target'], sub['target']))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:08:05.885529Z","iopub.execute_input":"2022-05-08T17:08:05.886264Z","iopub.status.idle":"2022-05-08T17:08:05.903287Z","shell.execute_reply.started":"2022-05-08T17:08:05.886222Z","shell.execute_reply":"2022-05-08T17:08:05.902594Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"AUC","metadata":{"execution":{"iopub.status.busy":"2022-05-08T17:08:05.904560Z","iopub.execute_input":"2022-05-08T17:08:05.904885Z","iopub.status.idle":"2022-05-08T17:08:05.911124Z","shell.execute_reply.started":"2022-05-08T17:08:05.904847Z","shell.execute_reply":"2022-05-08T17:08:05.910070Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0.8551436007488242"},"metadata":{}}]}]}